{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shimo23333/generative_ai/blob/main/AI%E6%9C%9F%E6%9C%AB_0603%E8%B7%A8%E6%96%87%E5%8C%96%E8%89%B2%E5%BD%A9%E8%AA%9E%E6%84%8F%E6%AF%94%E8%BC%83_CLIP%E4%B8%AD%E8%AA%9E%E8%A8%80%E5%B0%8D%E8%89%B2%E5%BD%A9%E7%90%86%E8%A7%A3%E7%9A%84%E5%B7%AE%E7%95%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b5MppSXtAZe",
        "outputId": "040f17f2-b240-426a-8beb-afb832adf7f4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.42 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.81.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.16.1 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m1:已完成。\n"
          ]
        }
      ],
      "source": [
        "#1:安裝\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "!pip install -q googletrans==4.0.0rc1 # 確保是這個版本或能正常工作的版本\n",
        "!pip install -q scikit-image\n",
        "!pip install -q opencv-python\n",
        "!pip install -q diffusers transformers accelerate invisible-watermark safetensors\n",
        "!pip install -q ipywidgets\n",
        "print(\"1:已完成。\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2:匯入\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from skimage.color import rgb2lab\n",
        "import cv2\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import matplotlib.font_manager as fm\n",
        "from transformers import pipeline\n",
        "import re # 用於生成 concept_name\n",
        "\n",
        "print(\"2:函式庫匯入完成。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsQlITEMtKou",
        "outputId": "29db308b-02ca-4093-f7fa-719ac1d0d37f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2:函式庫匯入完成。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#這是一個用來設定「整個專案環境」的類別，自動檢查是不是能用GPU、設定好畫圖用的中文字體，同時確保圖像要存的資料夾已經建立好。\n",
        "#原本是ai生成的但已經過我的理解與整理\n",
        "class ConfigManager:\n",
        "    def __init__(self, font_path_cjk='/usr/share/fonts/opentype/noto/NotoSansCJKjp-Regular.otf',\n",
        "                 images_out_dir=\"project_outputs_final_v4_oop_hf\"): # 修改輸出目錄名以區分\n",
        "        self.device = self._get_device()\n",
        "        self.font_path_cjk = font_path_cjk\n",
        "        self.images_out_dir = images_out_dir\n",
        "        self._setup_matplotlib_font()\n",
        "        self._setup_output_directory()\n",
        "        self.hf_device_id = 0 if self.device == \"cuda\" else -1\n",
        "\n",
        "    def _get_device(self):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"4:本次運行的計算設備是: {device}\")\n",
        "        if device == \"cpu\":\n",
        "            print(\"警告：未使用GPU！運行大型AI模型會非常慢。\")\n",
        "        return device\n",
        "\n",
        "    def _setup_matplotlib_font(self):\n",
        "        print(\"3:正在設定 Matplotlib CJK 字體...\")\n",
        "        if os.path.exists(self.font_path_cjk):\n",
        "            try:\n",
        "                fm.fontManager.addfont(self.font_path_cjk)\n",
        "                prop = fm.FontProperties(fname=self.font_path_cjk)\n",
        "                font_name = prop.get_name()\n",
        "                plt.rcParams['font.family'] = font_name\n",
        "                plt.rcParams['axes.unicode_minus'] = False\n",
        "                print(f\"  已成功設定 CJK 字體為: {font_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  設定字體 '{self.font_path_cjk}' 時發生錯誤: {e}. 退到通用列表。\")\n",
        "                plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'sans-serif']\n",
        "        else:\n",
        "            print(f\"  指定的 CJK 字體文件路徑不存在: {self.font_path_cjk}. 退到通用列表。\")\n",
        "            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'sans-serif']\n",
        "        plt.rcParams['axes.unicode_minus'] = False\n",
        "        print(\"3:Matplotlib CJK 字體設定完畢。\")\n",
        "\n",
        "    def _setup_output_directory(self):\n",
        "        if not os.path.exists(self.images_out_dir):\n",
        "            os.makedirs(self.images_out_dir)\n",
        "            print(f\"  已建立圖像儲存目錄: {self.images_out_dir}\")\n"
      ],
      "metadata": {
        "id": "Iv48bKSCvB41"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#這個類別是用來處理「各種模型的載入與管理」的，幫我把三種重要的模型準備好，CLIP、Stable Diffusion、翻譯模型\n",
        "#原本是ai生成的但已經過我的理解與整理，並且也自己找了應該可行的模型\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, device, hf_device_id):\n",
        "        self.device = device\n",
        "        self.hf_device_id = hf_device_id\n",
        "        self.clip_model = None\n",
        "        self.clip_preprocess = None\n",
        "        self.sd_pipeline = None\n",
        "        self.translation_pipelines = {}\n",
        "\n",
        "    def load_clip_model(self, model_name=\"ViT-B/32\"):\n",
        "        print(f\"5:準備載入 CLIP 模型 ({model_name})...\")\n",
        "        if self.device == \"cuda\":\n",
        "            try:\n",
        "                self.clip_model, self.clip_preprocess = clip.load(model_name, device=self.device)\n",
        "                self.clip_model.eval()\n",
        "                print(f\"  CLIP 模型 ({model_name}) 已成功載入到 {self.device}！\")\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                print(f\"  載入CLIP模型時發生錯誤: {e}\")\n",
        "                self.clip_model, self.clip_preprocess = None, None\n",
        "        else:\n",
        "            print(f\"  未實際載入CLIP模型，因為當前運算設備是 {self.device}。\")\n",
        "        return self.clip_model, self.clip_preprocess\n",
        "\n",
        "    def load_sd_model(self, model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
        "        print(f\"6:準備載入 Stable Diffusion 模型 ({model_id})...\")\n",
        "        if self.device == \"cuda\":\n",
        "            try:\n",
        "                self.sd_pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "                self.sd_pipeline = self.sd_pipeline.to(self.device)\n",
        "                print(f\"  Stable Diffusion 模型 ({model_id}) 已成功載入到 {self.device}！\")\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                print(f\"  載入Stable Diffusion模型時發生錯誤: {e}\")\n",
        "                self.sd_pipeline = None\n",
        "        else:\n",
        "            print(f\"  未實際載入Stable Diffusion模型，因為當前運算設備是 {self.device}。\")\n",
        "        return self.sd_pipeline\n",
        "\n",
        "    def init_translation_models(self, target_languages=['en', 'ja', 'ko']):\n",
        "        print(\"7:正在初始化 Hugging Face 翻譯模型...\")\n",
        "        # 你可能需要根據你的中文輸入類型（簡體/繁體）選擇更合適的模型\n",
        "        # 例如 'Helsinki-NLP/opus-mt-ZH-PLACEHOLDER' (ZH 代表廣泛中文)\n",
        "        # 或者確保你的輸入與模型期望的中文方言一致\n",
        "        model_map = {\n",
        "            'en': 'Helsinki-NLP/opus-mt-zh-en',\n",
        "            'ja': 'Helsinki-NLP/opus-mt-zh-ja', # 假設存在且適用\n",
        "            'ko': 'Helsinki-NLP/opus-mt-zh-ko'  # 假設存在且適用\n",
        "        }\n",
        "        loaded_any_model = False\n",
        "        for lang_code in target_languages:\n",
        "            if lang_code in model_map:\n",
        "                model_name = model_map[lang_code]\n",
        "                try:\n",
        "                    print(f\"  載入翻譯模型 for zh -> {lang_code} ({model_name})...\")\n",
        "                    translator = pipeline(f\"translation_zh_to_{lang_code}\", # 任務名可能需要調整\n",
        "                                          model=model_name,\n",
        "                                          device=self.hf_device_id)\n",
        "                    self.translation_pipelines[lang_code] = translator\n",
        "                    print(f\"    翻譯模型 for zh -> {lang_code} ({model_name}) 載入成功。\")\n",
        "                    loaded_any_model = True\n",
        "                except Exception as e:\n",
        "                    print(f\"    載入翻譯模型 for zh -> {lang_code} ({model_name}) 失敗: {e}\")\n",
        "                    self.translation_pipelines[lang_code] = None\n",
        "            else:\n",
        "                print(f\"  未找到針對 zh -> {lang_code} 的預定義翻譯模型。\")\n",
        "        if not loaded_any_model:\n",
        "            print(\"警告: 未能成功載入任何Hugging Face翻譯模型。自動翻譯功能將受限。\")\n",
        "        return self.translation_pipelines\n",
        "\n",
        "    def cleanup(self):\n",
        "        print(\"正在清理模型資源...\")\n",
        "        if self.clip_model: del self.clip_model\n",
        "        if self.clip_preprocess: del self.clip_preprocess\n",
        "        if self.sd_pipeline: del self.sd_pipeline\n",
        "        if self.translation_pipelines:\n",
        "            for lang, pipe in self.translation_pipelines.items():\n",
        "                if pipe: del pipe # 釋放pipeline物件\n",
        "            self.translation_pipelines.clear()\n",
        "            if self.device == \"cuda\": # 清理pipeline可能佔用的VRAM\n",
        "                torch.cuda.empty_cache()\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"  模型和資源清理操作已執行。\")"
      ],
      "metadata": {
        "id": "WGwczTjNvDFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#用來處理:詞彙概念的建立與管理，\"希望能\"幫我把一個「中文概念」翻譯成多種語言版本（中、英、日、韓）\n",
        "#並且記錄每個單字、原始中文、和各語言的詞彙說明(這部分一直做不太成功)\n",
        "#總之就是自動翻譯，也請ai幫我做過好幾版，結果都不太好，又經過好幾次自己手動改跟摸索......\n",
        "\n",
        "class ConceptDataProvider:\n",
        "    def __init__(self, clear_default_concepts=False): # 新增參數\n",
        "        self.word_concepts_list = [] # 默認清空，由用戶在main中添加\n",
        "        if not clear_default_concepts:\n",
        "            # 如果需要，可以保留原有的預設列表\n",
        "            self.word_concepts_list = [\n",
        "                {\"concept_name\": \"cool_ambiguous\", \"base_chinese\": \"酷 / 涼爽\", \"translations\": {\"zh\": \"冰涼的飲料，酷炫的風格，冷靜的態度\", \"en\": \"cool refreshing drink, cool stylish look, calm and cool attitude\", \"ja\": \"冷たい飲み物、かっこいいスタイル、冷静な態度\", \"ko\": \"시원한 음료, 멋진 스타일, 침착한 태도\"}},\n",
        "                # ... (其他預設概念可以放在這裡)\n",
        "            ]\n",
        "        self.word_concepts_to_process = self.word_concepts_list\n",
        "        print(f\"8:初始定義了 {len(self.word_concepts_list)} 個詞彙概念。\")\n",
        "\n",
        "    def _generate_concept_name(self, base_chinese):\n",
        "        \"\"\"根據中文詞彙生成一個簡化的英文概念名\"\"\"\n",
        "        # 移除特殊字符，只保留字母和數字，並用下劃線連接\n",
        "        # 這是一個非常基礎的實現，可能需要更複雜的邏輯來生成好的英文名\n",
        "        # 或者，要求用戶為每個詞彙也提供一個英文 concept_name\n",
        "        name = re.sub(r'[^\\w]', '', base_chinese) # 移除非字母數字字符\n",
        "        name = name[:20] # 限制長度\n",
        "        if not name: name = \"unnamed_concept\"\n",
        "        return f\"{name}_auto_translated\"\n",
        "\n",
        "\n",
        "    def add_concept_with_auto_translation(self, translation_pipelines, base_chinese,\n",
        "                                          chinese_description_for_prompt=None, target_languages=['en', 'ja', 'ko']):\n",
        "        \"\"\"\n",
        "        新增一個概念，並使用 Hugging Face pipeline 自動翻譯其描述性句子。\n",
        "        如果 chinese_description_for_prompt 未提供，則直接翻譯 base_chinese。\n",
        "        \"\"\"\n",
        "        concept_name = self._generate_concept_name(base_chinese) # 自動生成 concept_name\n",
        "        text_to_translate = chinese_description_for_prompt if chinese_description_for_prompt else base_chinese\n",
        "\n",
        "        print(f\"\\n  準備新增概念 '{concept_name}' (基於 '{base_chinese}')...\")\n",
        "        print(f\"    將翻譯: '{text_to_translate[:50]}...'\")\n",
        "\n",
        "        if not translation_pipelines or not any(translation_pipelines.values()):\n",
        "            print(\"    錯誤：翻譯 pipelines 未提供或均未成功載入，無法自動翻譯。將使用原文作為提示。\")\n",
        "            translations = {lang: text_to_translate for lang in target_languages}\n",
        "            translations['zh'] = text_to_translate\n",
        "        else:\n",
        "            translations = {\"zh\": text_to_translate} # 中文提示使用原始描述或詞彙\n",
        "            for lang_code in target_languages:\n",
        "                translator_pipeline = translation_pipelines.get(lang_code)\n",
        "                if translator_pipeline:\n",
        "                    try:\n",
        "                        translated_result = translator_pipeline(text_to_translate)\n",
        "                        translated_text = translated_result[0]['translation_text']\n",
        "                        translations[lang_code] = translated_text\n",
        "                        print(f\"      -> {lang_code.upper()}: {translated_text}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"      使用Hugging Face模型翻譯到 {lang_code.upper()} 失敗: {e}\")\n",
        "                        translations[lang_code] = f\"翻譯失敗: {text_to_translate}\" # 回退\n",
        "                else:\n",
        "                    print(f\"      未找到 {lang_code.upper()} 的翻譯 pipeline，使用原文。\")\n",
        "                    translations[lang_code] = text_to_translate # 回退\n",
        "\n",
        "        new_concept = {\n",
        "            \"concept_name\": concept_name,\n",
        "            \"base_chinese\": base_chinese, # 核心詞彙\n",
        "            \"translations\": translations  # 用於 prompt 的翻譯文本\n",
        "        }\n",
        "        self.word_concepts_list.append(new_concept)\n",
        "        print(f\"  新概念 '{concept_name}' 已成功添加。現有 {len(self.word_concepts_list)} 個概念。\")\n",
        "        # 更新 self.word_concepts_to_process，因為它與 self.word_concepts_list 是同一個物件\n",
        "        print(f\"8(更新):定義了 {len(self.word_concepts_to_process)} 個詞彙概念用於本次分析。\")\n",
        "\n",
        "    def get_concepts_to_process(self):\n",
        "        if not self.word_concepts_to_process:\n",
        "            print(\"警告：沒有定義任何詞彙概念進行處理。\")\n",
        "        return self.word_concepts_to_process\n"
      ],
      "metadata": {
        "id": "w4a_ashAvGdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#主要處理CLIP的文本分析功能\n",
        "#作用1.把多語言的提示詞送進CLIP模型，轉換成向量\n",
        "#作用2.比較不同語言之間的語意相似度\n",
        "#希望能以此判斷「翻譯後的提示詞」在語意上是不是相近\n",
        "#這段原本也是ai生成的，經過我的理解與整理:\n",
        "\n",
        "class TextAnalyzer:\n",
        "    def __init__(self, clip_model, device):\n",
        "        self.clip_model = clip_model\n",
        "        self.device = device\n",
        "    def get_clip_text_embeddings(self, text_prompts_dict):\n",
        "        if self.clip_model is None:\n",
        "            print(\"  警告: CLIP 模型未載入，文本嵌入將為零向量。\")\n",
        "            return {lang_code: np.zeros(512, dtype=np.float32) for lang_code in text_prompts_dict}\n",
        "        text_embeddings_result_dict = {}\n",
        "        with torch.no_grad():\n",
        "            for lang_tag, text_content in text_prompts_dict.items():\n",
        "                try:\n",
        "                    tokenized_input_text = clip.tokenize([text_content]).to(self.device)\n",
        "                    text_semantic_features = self.clip_model.encode_text(tokenized_input_text)\n",
        "                    text_semantic_features /= text_semantic_features.norm(dim=-1, keepdim=True)\n",
        "                    text_embeddings_result_dict[lang_tag] = text_semantic_features.cpu().numpy().flatten()\n",
        "                except Exception as e:\n",
        "                    print(f\"為 '{lang_tag}':'{text_content[:30]}...' 生成CLIP嵌入時出錯: {e}\")\n",
        "                    text_embeddings_result_dict[lang_tag] = np.zeros(512, dtype=np.float32)\n",
        "        return text_embeddings_result_dict\n",
        "    def calculate_embedding_similarity(self, embeddings_dict, reference_lang='en'):\n",
        "        similarity_scores_result = {}\n",
        "        if reference_lang not in embeddings_dict or embeddings_dict.get(reference_lang) is None or np.all(np.isclose(embeddings_dict[reference_lang], 0)):\n",
        "            print(f\"  參考語言 '{reference_lang.upper()}' 的嵌入向量無效或不存在，無法計算相似度。\")\n",
        "            return {f\"{reference_lang}_vs_{lang}\": None for lang in embeddings_dict if lang != reference_lang}\n",
        "        print(f\"  CLIP文本嵌入向量餘弦相似度 (vs '{reference_lang.upper()}'):\")\n",
        "        ref_embedding = embeddings_dict[reference_lang].reshape(1, -1)\n",
        "        for lang, emb in embeddings_dict.items():\n",
        "            if lang == reference_lang: continue\n",
        "            sim_val_str = \"N/A (嵌入無效)\"\n",
        "            sim_num = None\n",
        "            if emb is not None and not np.all(np.isclose(emb, 0)):\n",
        "                sim_num = cosine_similarity(ref_embedding, emb.reshape(1, -1))[0][0]\n",
        "                sim_val_str = f\"{sim_num:.3f}\"\n",
        "            similarity_scores_result[f\"{reference_lang}_vs_{lang}\"] = sim_num\n",
        "            print(f\"    - 與 {lang.upper()}: {sim_val_str}\")\n",
        "        return similarity_scores_result"
      ],
      "metadata": {
        "id": "Dh5nle1GvHpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#處理「圖像生成與分析」的功能\n",
        "#作用1.用Stable Diffusion模型根據提示詞生成圖片(目前成效不佳)\n",
        "#作用2.圖片中提取出主色\n",
        "#作用3.分析圖片的整體亮度、對比、飽和度\n",
        "\n",
        "class ImageProcessor:\n",
        "    def __init__(self, sd_pipeline, device):\n",
        "        self.sd_pipeline = sd_pipeline\n",
        "        self.device = device\n",
        "    def generate_image_with_sd(self, prompt_text, random_seed=42, inference_steps=30, cfg_scale=7.5):\n",
        "        if self.sd_pipeline is None:\n",
        "            placeholder_img = Image.new('RGB', (512, 512), color='silver')\n",
        "            draw = ImageDraw.Draw(placeholder_img)\n",
        "            try: font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n",
        "            except IOError: font = ImageFont.load_default()\n",
        "            draw.text((10, 10), f\"SD模型未載入\\n提示:\\n{prompt_text[:70]}...\", fill=(60, 60, 60), font=font)\n",
        "            return placeholder_img\n",
        "        try:\n",
        "            gen = torch.Generator(device=self.device).manual_seed(random_seed)\n",
        "            with torch.no_grad():\n",
        "                img = self.sd_pipeline(prompt_text, num_inference_steps=inference_steps, guidance_scale=cfg_scale, generator=gen).images[0]\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"  生成圖像時出錯 ('{prompt_text[:40]}...'): {e}\")\n",
        "            error_img = Image.new('RGB', (512, 512), color='lightcoral')\n",
        "            draw = ImageDraw.Draw(error_img)\n",
        "            try: font = ImageFont.truetype(\"DejaVuSans.ttf\", 15)\n",
        "            except IOError: font = ImageFont.load_default()\n",
        "            draw.text((10, 10), f\"圖像生成錯誤:\\n{prompt_text[:60]}...\\n錯誤:\\n{str(e)[:100]}\", fill=(0, 0, 0), font=font)\n",
        "            return error_img\n",
        "    def extract_dominant_colors(self, pil_img, num_colors=5):\n",
        "        if pil_img is None or (hasattr(pil_img, 'width') and pil_img.width < num_colors) or \\\n",
        "           (hasattr(pil_img, 'height') and pil_img.height < num_colors): # 更安全的檢查\n",
        "            rgb_fallback = np.array([[128, 128, 128]] * num_colors, dtype=int)\n",
        "            lab_fallback = rgb2lab(rgb_fallback.reshape((num_colors, 1, 3)) / 255.0).reshape((num_colors, 3))\n",
        "            return rgb_fallback, lab_fallback\n",
        "        try:\n",
        "            img_rgb = pil_img.convert('RGB')\n",
        "            max_dim = 150\n",
        "            ratio = max_dim / max(img_rgb.width, img_rgb.height)\n",
        "            new_size = (max(1, int(img_rgb.width * ratio)), max(1, int(img_rgb.height * ratio)))\n",
        "            img_res = img_rgb.resize(new_size, Image.Resampling.LANCZOS)\n",
        "            pixels = np.array(img_res).reshape(-1, 3)\n",
        "            if pixels.shape[0] < num_colors:\n",
        "                rgb_colors = np.zeros((num_colors, 3), dtype=int)\n",
        "                actual_extracted_colors = pixels.astype(int)\n",
        "                rgb_colors[:actual_extracted_colors.shape[0]] = actual_extracted_colors\n",
        "                if actual_extracted_colors.shape[0] < num_colors:\n",
        "                    rgb_colors[actual_extracted_colors.shape[0]:] = np.array([128,128,128])\n",
        "            else:\n",
        "                kmeans = KMeans(n_clusters=num_colors, random_state=0, n_init='auto', max_iter=200).fit(pixels)\n",
        "                rgb_colors = kmeans.cluster_centers_.astype(int)\n",
        "            if rgb_colors.shape[0] < num_colors:\n",
        "                padded_colors = np.full((num_colors, 3), 128, dtype=int)\n",
        "                padded_colors[:rgb_colors.shape[0]] = rgb_colors\n",
        "                rgb_colors = padded_colors\n",
        "            lab_colors = rgb2lab(rgb_colors.reshape((num_colors, 1, 3)) / 255.0).reshape((num_colors, 3))\n",
        "            return rgb_colors, lab_colors\n",
        "        except Exception as e:\n",
        "            print(f\"  提取主色調時出錯: {e}\")\n",
        "            rgb_err = np.array([[100, 100, 100]] * num_colors, dtype=int)\n",
        "            lab_err = rgb2lab(rgb_err.reshape((num_colors,1,3)) / 255.0).reshape((num_colors, 3))\n",
        "            return rgb_err, lab_err\n",
        "    def analyze_global_features(self, pil_image):\n",
        "        if pil_image is None: return {\"avg_brightness\": \"N/A\", \"contrast_std\": \"N/A\", \"avg_saturation\": \"N/A\"}\n",
        "        try:\n",
        "            cv_bgr = np.array(pil_image.convert('RGB'))[:, :, ::-1].copy()\n",
        "            gray = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2GRAY)\n",
        "            brightness = round(np.mean(gray), 2)\n",
        "            contrast = round(np.std(gray), 2)\n",
        "            hsv = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2HSV)\n",
        "            saturation = round(np.mean(hsv[:, :, 1]), 2)\n",
        "            return {\"avg_brightness\": brightness, \"contrast_std\": contrast, \"avg_saturation\": saturation}\n",
        "        except Exception as e:\n",
        "            print(f\"  分析全局圖像特徵時出錯: {e}\")\n",
        "            return {\"avg_brightness\": \"Err\", \"contrast_std\": \"Err\", \"avg_saturation\": \"Err\"}"
      ],
      "metadata": {
        "id": "v1Dmo3XwvI8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#分析報告\n",
        "#作用1.根據概念和分析結果，產出文字說明框架\n",
        "#作用2.把圖片、顏色、CLIP相似度、特徵等等畫成一張完整比較圖\n",
        "#以上就是文字理解~圖像生成~結果分析~報告輸出\n",
        "\n",
        "class ReportGenerator:\n",
        "    def generate_explanation_template(self, chinese_concept, lang_prompt_info, dom_colors_hex=None, global_feats=None):\n",
        "        expl = f\"\\n--- 解釋模板 for 概念:【{chinese_concept}】| 語言提示: 【{lang_prompt_info[:70]}...】 ---\\n\"\n",
        "        if dom_colors_hex: expl += f\"圖像主要色票 (HEX): {', '.join(dom_colors_hex[:3])} ...\\n\"\n",
        "        if global_feats:\n",
        "            expl += f\"全局圖像特徵: 亮度={global_feats.get('avg_brightness', 'N/A')}, \"\n",
        "            expl += f\"對比度={global_feats.get('contrast_std', 'N/A')}, 飽和度={global_feats.get('avg_saturation', 'N/A')}\\n\"\n",
        "        expl += f\"\\n原因推測與圖像描述 (請您填充)：\\n\"\n",
        "        expl += f\"   [請結合以上客觀指標和您的觀察，詳細闡述：\\n\"\n",
        "        expl += f\"    a. 圖像視覺風格與氛圍？\\n    b. 主要元素與提示詞的關聯？\\n    c. 色彩運用如何詮釋提示詞？\\n\"\n",
        "        expl += f\"    d. (特定語言)文化背景的可能影響？\\n    e. 與其他語言生成圖像的差異及可能原因？]\\n\"\n",
        "        expl += f\"--------------------------------------------------------------------------\\n\"\n",
        "        return expl\n",
        "    def plot_concept_results(self, concept_id, base_chinese, prompts_dict, images_dict,colors_dict, similarities_dict, global_features_dict=None):\n",
        "        langs = list(prompts_dict.keys())\n",
        "        num_langs = len(langs)\n",
        "        if num_langs == 0:\n",
        "            print(f\"概念 '{concept_id}' 無 prompts 可繪製。\")\n",
        "            return\n",
        "        h_ratio, w_ratio = 2.8, 4.0\n",
        "        total_h, total_w = h_ratio * 2, w_ratio * num_langs if num_langs > 0 else w_ratio\n",
        "        fig, axs = plt.subplots(2, max(1, num_langs), figsize=(total_w, total_h), gridspec_kw={'height_ratios': [0.78, 0.22]})\n",
        "        if num_langs == 1: axs = axs.reshape(2, 1)\n",
        "\n",
        "        title_base = f\"概念分析: '{base_chinese}' ({concept_id})\\nCLIP相似度(vs EN): \"\n",
        "        sim_strs = []\n",
        "        if similarities_dict:\n",
        "            sim_strs = [f\"{k.split('_vs_')[-1].upper()}: {v:.2f}\" if isinstance(v, (float, np.floating)) else f\"{k.split('_vs_')[-1].upper()}: {v}\"\n",
        "                        for k, v in similarities_dict.items()]\n",
        "        fig.suptitle(title_base + \", \".join(sim_strs), fontsize=11, y=1.04)\n",
        "        for i, lang in enumerate(langs):\n",
        "            img = images_dict.get(lang)\n",
        "            colors_data = colors_dict.get(lang)\n",
        "            global_feats_this_lang = (global_features_dict or {}).get(lang, {})\n",
        "            ax_img = axs[0, i]; ax_color = axs[1, i]\n",
        "            if img: ax_img.imshow(img)\n",
        "            else: ax_img.text(0.5, 0.5, '圖像未生成', ha='center', va='center', transform=ax_img.transAxes)\n",
        "            img_title_prompt = prompts_dict.get(lang, \"N/A\")\n",
        "            img_title = f\"{lang.upper()}: \\\"{img_title_prompt[:30]}\\\"...\"\n",
        "            if global_feats_this_lang:\n",
        "                img_title += f\"\\n亮:{global_feats_this_lang.get('avg_brightness', '-')} 對比:{global_feats_this_lang.get('contrast_std', '-')} 飽:{global_feats_this_lang.get('avg_saturation', '-')}\"\n",
        "            ax_img.set_title(img_title, fontsize=7.5); ax_img.axis('off')\n",
        "            if colors_data:\n",
        "                rgb_patch, lab_patch = colors_data\n",
        "                if rgb_patch is not None and len(rgb_patch) > 0 :\n",
        "                    n_patch = len(rgb_patch)\n",
        "                    patch_canvas = np.zeros((25, 100, 3), dtype=np.uint8)\n",
        "                    patch_w = 100 // n_patch\n",
        "                    for j, rgb_c in enumerate(rgb_patch): patch_canvas[:, j * patch_w:(j + 1) * patch_w] = rgb_c\n",
        "                    ax_color.imshow(patch_canvas)\n",
        "                    lab_str_parts = []\n",
        "                    if lab_patch is not None:\n",
        "                        for l_val, a_val, b_val in lab_patch[:min(3,n_patch)]: lab_str_parts.append(f\"L{l_val:.0f} a{a_val:.0f} b{b_val:.0f}\")\n",
        "                        lab_str = \"\\n\".join(lab_str_parts)\n",
        "                        ax_color.set_title(f\"Lab(Top{min(3,n_patch)}):\\n{lab_str}\", fontsize=6)\n",
        "                    else: ax_color.set_title(f\"RGB顏色\", fontsize=6)\n",
        "                else: ax_color.text(0.5,0.5,'無顏色數據',ha='center',va='center',transform=ax_color.transAxes, fontsize=6)\n",
        "            else: ax_color.text(0.5, 0.5, '無顏色', ha='center', va='center', transform=ax_color.transAxes, fontsize=6)\n",
        "            ax_color.axis('off')\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.94]); plt.subplots_adjust(hspace=0.5, wspace=0.3); plt.show()"
      ],
      "metadata": {
        "id": "NGGBRHnJvKVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#串接所有模組(文字分析+圖像處理+報告生成)\n",
        "#除錯誤加了if prompt is None:#遇到翻譯缺失就跳\n",
        "\n",
        "class AnalysisPipeline:\n",
        "    def __init__(self, config_manager, model_manager, concept_provider, text_analyzer, image_processor, report_generator):\n",
        "        self.config = config_manager; self.models = model_manager; self.concepts_provider = concept_provider\n",
        "        self.text_analyzer = text_analyzer; self.image_processor = image_processor; self.reporter = report_generator\n",
        "        self.base_seed = 20240101; self.sd_steps = 20; self.sd_cfg = 7.0 # 減少步數以加速\n",
        "        self.num_dom_colors = 5; self.save_images_flag = True\n",
        "    def run_analysis(self):\n",
        "        print(f\"Cell 15: 即將開始執行主流程...\")\n",
        "        concepts_to_process = self.concepts_provider.get_concepts_to_process()\n",
        "        if not concepts_to_process:\n",
        "            print(\"沒有概念需要處理，流程結束。\")\n",
        "            return []\n",
        "        print(f\"  將處理 {len(concepts_to_process)} 個詞彙概念...\")\n",
        "        results_collection = []\n",
        "        for concept_idx, concept_detail in enumerate(tqdm(concepts_to_process, desc=\"總體概念處理\")):\n",
        "            concept_id = concept_detail[\"concept_name\"]; base_zh = concept_detail[\"base_chinese\"]; prompts = concept_detail[\"translations\"]\n",
        "            print(f\"\\n\\n處理概念 #{concept_idx + 1}: '{base_zh}' ({concept_id})\")\n",
        "            print(\"  [1. CLIP嵌入分析]\")\n",
        "            embeddings = self.text_analyzer.get_clip_text_embeddings(prompts)\n",
        "            similarities = self.text_analyzer.calculate_embedding_similarity(embeddings, reference_lang='en')\n",
        "            print(\"  [2. 圖像生成、顏色與全局特徵分析]\")\n",
        "            concept_images = {}; concept_colors = {}; concept_global_features = {}; concept_explanations_str = \"\"\n",
        "            for lang_idx, (lang, prompt) in enumerate(tqdm(prompts.items(), desc=f\"  '{concept_id}'語言處理\", leave=False)):\n",
        "                print(f\"    -> {lang.upper()}: '{prompt}'\")\n",
        "                img_seed = self.base_seed + concept_idx * 100 + lang_idx * 10\n",
        "                pil_img = self.image_processor.generate_image_with_sd(prompt, random_seed=img_seed, inference_steps=self.sd_steps, cfg_scale=self.sd_cfg)\n",
        "                concept_images[lang] = pil_img\n",
        "                if self.save_images_flag and pil_img:\n",
        "                    try:\n",
        "                        fname = f\"{concept_id}_{lang}_s{img_seed}.png\"; fpath = os.path.join(self.config.images_out_dir, fname)\n",
        "                        pil_img.save(fpath)\n",
        "                    except Exception as e: print(f\"      儲存圖像'{fname}'失敗: {e}\")\n",
        "                rgb_cs, lab_cs = self.image_processor.extract_dominant_colors(pil_img, self.num_dom_colors)\n",
        "                concept_colors[lang] = (rgb_cs, lab_cs)\n",
        "                global_feats = self.image_processor.analyze_global_features(pil_img)\n",
        "                concept_global_features[lang] = global_feats\n",
        "                hex_colors = [f\"#{c[0]:02x}{c[1]:02x}{c[2]:02x}\" for c in rgb_cs] if rgb_cs is not None and len(rgb_cs)>0 else []\n",
        "                expl_text = self.reporter.generate_explanation_template(base_zh, f\"{lang.upper()}: {prompt}\", hex_colors, global_feats)\n",
        "                print(expl_text); concept_explanations_str += expl_text\n",
        "                if self.config.device == \"cuda\": torch.cuda.empty_cache(); time.sleep(0.05)\n",
        "            print(\"\\n  [3. 繪製結果圖表]\")\n",
        "            self.reporter.plot_concept_results(concept_id, base_zh, prompts, concept_images, concept_colors, similarities, concept_global_features)\n",
        "            results_collection.append({\"concept\": concept_id, \"base_chinese\": base_zh, \"prompts\": prompts, \"similarities\": similarities, \"global_features\": concept_global_features, \"explanation_prompts_combined\": concept_explanations_str})\n",
        "            print(f\"  概念 '{concept_id}' 分析完畢。\")\n",
        "            if self.config.device == \"cuda\": torch.cuda.empty_cache()\n",
        "        print(\"\\n\\n所有詞彙概念處理完成！解釋模板已在上方打印。\")\n",
        "        return results_collection"
      ],
      "metadata": {
        "id": "jOp8Pt8zvLg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 主執行流程\n",
        "if __name__ == '__main__':\n",
        "    # 1. 初始化配置管理器\n",
        "    config_mgr = ConfigManager()\n",
        "\n",
        "    # 2. 初始化模型管理器\n",
        "    model_mgr = ModelManager(device=config_mgr.device, hf_device_id=config_mgr.hf_device_id)\n",
        "    clip_model, _ = model_mgr.load_clip_model()\n",
        "    sd_pipeline = model_mgr.load_sd_model()\n",
        "    translation_pipelines = model_mgr.init_translation_models(target_languages=['en', 'ja', 'ko'])\n",
        "\n",
        "    # 3. 初始化數據提供者，並清空預設概念\n",
        "    concept_provider = ConceptDataProvider(clear_default_concepts=True)\n",
        "\n",
        "    # --- 定義你想要自動翻譯和分析的中文詞彙列表 ---\n",
        "    # 每個元素可以是：\n",
        "    #   - 一個簡單的中文詞 (str): 程式會直接翻譯這個詞作為各語言的 prompt\n",
        "    #   - 一個元組 (str, str): (核心中文詞, 用於翻譯生成 prompt 的中文描述句)\n",
        "    chinese_terms_to_analyze = [\n",
        "        \"酷\",\n",
        "        \"柔軟\",\n",
        "        \"明亮\",\n",
        "        \"黑暗\",\n",
        "        \"純潔\",\n",
        "        \"溫暖\",\n",
        "        \"快樂\",\n",
        "        \"生氣\",\n",
        "        \"傷心\",\n",
        "        \"驚訝\",\n",
        "        \"餓\",\n",
        "        \"疲憊\"\n",
        "    ]\n",
        "    # --- -------------------------------------- ---\n",
        "\n",
        "    print(f\"\\n--- 開始為用戶定義的 {len(chinese_terms_to_analyze)} 個詞彙添加概念 ---\")\n",
        "    if not any(translation_pipelines.values()): # 再次檢查翻譯模型是否真的載入\n",
        "        print(\"警告：沒有可用的翻譯模型，將主要使用中文原文作為提示。\")\n",
        "\n",
        "    for term_index, term_input in enumerate(chinese_terms_to_analyze):\n",
        "        core_chinese_word = \"\"\n",
        "        chinese_description = None\n",
        "\n",
        "        if isinstance(term_input, str):\n",
        "            core_chinese_word = term_input\n",
        "            # chinese_description = term_input # 如果希望直接翻譯詞彙，可以這樣設定\n",
        "            # 或者，你可以為單詞創建一個更豐富的預設描述模板\n",
        "            chinese_description = f\"一張描繪'{term_input}'的圖片，展現其典型特徵和氛圍\"\n",
        "        elif isinstance(term_input, tuple) and len(term_input) == 2:\n",
        "            core_chinese_word = term_input[0]\n",
        "            chinese_description = term_input[1]\n",
        "        else:\n",
        "            print(f\"跳過無效的輸入格式: {term_input}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  處理輸入 #{term_index + 1}: 核心詞='{core_chinese_word}', 描述='{chinese_description}'\")\n",
        "        concept_provider.add_concept_with_auto_translation(\n",
        "            translation_pipelines=translation_pipelines,\n",
        "            base_chinese=core_chinese_word,\n",
        "            chinese_description_for_prompt=chinese_description, # 傳遞描述句\n",
        "            target_languages=['en', 'ja', 'ko']\n",
        "        )\n",
        "    print(\"--- 用戶定義詞彙概念添加完成 ---\")\n",
        "\n",
        "    text_analyzer = TextAnalyzer(clip_model=clip_model, device=config_mgr.device)\n",
        "    image_processor = ImageProcessor(sd_pipeline=sd_pipeline, device=config_mgr.device)\n",
        "    report_generator = ReportGenerator()\n",
        "\n",
        "    # 4. 初始化並執行分析流程\n",
        "    pipeline = AnalysisPipeline(\n",
        "        config_manager=config_mgr,\n",
        "        model_manager=model_mgr,\n",
        "        concept_provider=concept_provider,\n",
        "        text_analyzer=text_analyzer,\n",
        "        image_processor=image_processor,\n",
        "        report_generator=report_generator\n",
        "    )\n",
        "\n",
        "    if not concept_provider.get_concepts_to_process():\n",
        "        print(\"沒有任何概念被定義，無法執行分析流程。請檢查 `chinese_terms_to_analyze`。\")\n",
        "    else:\n",
        "        all_results = pipeline.run_analysis()\n",
        "        # 可以選擇在這裡處理 all_results\n",
        "\n",
        "    # 5. 清理資源\n",
        "    model_mgr.cleanup()\n",
        "\n",
        "    print(\"腳本執行完畢。\")"
      ],
      "metadata": {
        "id": "cox86N5od_bY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}