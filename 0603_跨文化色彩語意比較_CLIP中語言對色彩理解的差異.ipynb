{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shimo23333/generative_ai/blob/main/0603_%E8%B7%A8%E6%96%87%E5%8C%96%E8%89%B2%E5%BD%A9%E8%AA%9E%E6%84%8F%E6%AF%94%E8%BC%83_CLIP%E4%B8%AD%E8%AA%9E%E8%A8%80%E5%B0%8D%E8%89%B2%E5%BD%A9%E7%90%86%E8%A7%A3%E7%9A%84%E5%B7%AE%E7%95%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b5MppSXtAZe",
        "outputId": "7915d720-bebf-4f20-c897-51b53766f322",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  fonts-noto-cjk-extra\n",
            "The following NEW packages will be installed:\n",
            "  fonts-noto-cjk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 61.2 MB of archives.\n",
            "After this operation, 93.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-cjk all 1:20220127+repack1-1 [61.2 MB]\n",
            "Fetched 61.2 MB in 2s (31.7 MB/s)\n",
            "Selecting previously unselected package fonts-noto-cjk.\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-noto-cjk_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Setting up fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ],
      "source": [
        "#1:安裝\n",
        "!pip install -q git+https://github.com/openai/CLIP.git #從GitHub安裝OpenAI的CLIP\n",
        "!pip install -q scikit-image #處理影像\n",
        "!pip install -q opencv-python #影像處理工具(OpenCV)\n",
        "!pip install -q diffusers transformers accelerate invisible-watermark safetensors #為了使用StableDiffusion\n",
        "!pip install -q ipywidgets\n",
        "!pip install -q sentencepiece sacremoses #翻譯模型\n",
        "!apt-get -y install fonts-noto-cjk #嘗試"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2:匯入\n",
        "import torch\n",
        "import clip #轉成向量\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans #用來找圖片裡最主要的幾個顏色\n",
        "from skimage.color import rgb2lab #把圖片從 RGB 顏色轉成 LAB 顏色空間的工具\n",
        "import cv2\n",
        "from diffusers import StableDiffusionPipeline #tableDiffusion\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity #比文字與圖像的語意是不是接近?\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import matplotlib.font_manager as fm\n",
        "from transformers import pipeline\n",
        "import re #生成concept_name\n",
        "\n",
        "print(\"2:函式庫匯入完成。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsQlITEMtKou",
        "outputId": "f1efe2bc-e704-4bd8-ad1d-ea3da24593fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2:函式庫匯入完成。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#這是一個用來設定「整個專案環境」的類別，自動檢查是不是能用GPU、設定好畫圖用的中文字體，同時確保圖像要存的資料夾已經建立好。\n",
        "#原本是ai生成的但已經過我的理解與整理\n",
        "class ConfigManager:\n",
        "    def __init__(self, font_path_cjk='/usr/share/fonts/opentype/noto/NotoSansCJKjp-Regular.otf',\n",
        "                 images_out_dir=\"project_outputs_final_v4_oop_hf\"):\n",
        "        self.device = self._get_device()\n",
        "        self.font_path_cjk = font_path_cjk\n",
        "        self.images_out_dir = images_out_dir\n",
        "        self._setup_matplotlib_font()\n",
        "        self._setup_output_directory()\n",
        "        self.hf_device_id = 0 if self.device == \"cuda\" else -1\n",
        "\n",
        "    def _get_device(self):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"4:本次運行的計算設備是: {device}\")\n",
        "        if device == \"cpu\":\n",
        "            print(\"警告：未使用GPU！運行大型AI模型會非常慢。\")\n",
        "        return device\n",
        "\n",
        "    def _setup_matplotlib_font(self):\n",
        "        print(\"3:正在設定 Matplotlib CJK 字體...\")\n",
        "        if os.path.exists(self.font_path_cjk):\n",
        "            try:\n",
        "                fm.fontManager.addfont(self.font_path_cjk)\n",
        "                prop = fm.FontProperties(fname=self.font_path_cjk)\n",
        "                font_name = prop.get_name()\n",
        "                plt.rcParams['font.family'] = font_name\n",
        "                plt.rcParams['axes.unicode_minus'] = False\n",
        "                print(f\"  已成功設定 CJK 字體為: {font_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  設定字體 '{self.font_path_cjk}' 時發生錯誤: {e}. 退到通用列表。\")\n",
        "                plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'SimHei', 'DejaVu Sans', 'sans-serif']\n",
        "        else:\n",
        "            print(f\"  指定的 CJK 字體文件路徑不存在: {self.font_path_cjk}. 退到通用列表。\")\n",
        "            plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'SimHei', 'DejaVu Sans', 'sans-serif']\n",
        "        plt.rcParams['axes.unicode_minus'] = False\n",
        "        print(\"3:Matplotlib CJK 字體設定完畢。\")\n",
        "\n",
        "    def _setup_output_directory(self):\n",
        "        if not os.path.exists(self.images_out_dir):\n",
        "            os.makedirs(self.images_out_dir)\n",
        "            print(f\"  已建立圖像儲存目錄: {self.images_out_dir}\")"
      ],
      "metadata": {
        "id": "Iv48bKSCvB41"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#這個類別是用來處理「各種模型的載入與管理」的，幫我把三種重要的模型準備好，CLIP、Stable Diffusion、翻譯模型\n",
        "#原本是ai生成的但已經過我的理解與整理，並且也自己找了應該可行的模型\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, device, hf_device_id):\n",
        "        self.device = device\n",
        "        self.hf_device_id = hf_device_id\n",
        "        self.clip_model = None\n",
        "        self.clip_preprocess = None\n",
        "        self.sd_pipeline = None\n",
        "        self.translation_pipelines = {}\n",
        "\n",
        "    def load_clip_model(self, model_name=\"ViT-B/32\"):\n",
        "        print(f\"5:準備載入 CLIP 模型 ({model_name})...\")\n",
        "        if self.device == \"cuda\":\n",
        "            try:\n",
        "                self.clip_model, self.clip_preprocess = clip.load(model_name, device=self.device)\n",
        "                self.clip_model.eval()\n",
        "                print(f\"  CLIP 模型 ({model_name}) 已成功載入到 {self.device}！\")\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                print(f\"  載入CLIP模型時發生錯誤: {e}\")\n",
        "                self.clip_model, self.clip_preprocess = None, None\n",
        "        else:\n",
        "            print(f\"  未實際載入CLIP模型，因為當前運算設備是 {self.device}。\")\n",
        "        return self.clip_model, self.clip_preprocess\n",
        "\n",
        "    def load_sd_model(self, model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
        "        print(f\"6:準備載入 Stable Diffusion 模型 ({model_id})...\")\n",
        "        if self.device == \"cuda\":\n",
        "            try:\n",
        "                self.sd_pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "                self.sd_pipeline = self.sd_pipeline.to(self.device)\n",
        "                print(f\"  Stable Diffusion 模型 ({model_id}) 已成功載入到 {self.device}！\")\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                print(f\"  載入Stable Diffusion模型時發生錯誤: {e}\")\n",
        "                self.sd_pipeline = None\n",
        "        else:\n",
        "            print(f\"  未實際載入Stable Diffusion模型，因為當前運算設備是 {self.device}。\")\n",
        "        return self.sd_pipeline\n",
        "\n",
        "    def init_translation_models(self, target_languages=['en', 'ja', 'ko']):\n",
        "        print(\"7:正在初始化 Hugging Face 翻譯模型...\")\n",
        "        model_map = {\n",
        "            'en': 'Helsinki-NLP/opus-mt-zh-en',\n",
        "            'ja': 'Helsinki-NLP/opus-mt-tc-big-zh-ja', # 網路上自己找到的模型\n",
        "            'ko': 'Helsinki-NLP/opus-mt-tc-big-zh-ja'  # 網路上自己找到的模型\n",
        "        }\n",
        "        loaded_any_model = False\n",
        "        for lang_code in target_languages:\n",
        "            if lang_code in model_map:\n",
        "                model_name = model_map[lang_code]\n",
        "                try:\n",
        "                    print(f\"  載入翻譯模型 for zh -> {lang_code} ({model_name})...\")\n",
        "                    translator = pipeline(f\"translation_zh_to_{lang_code}\", # 任務名可能需要調整\n",
        "                                          model=model_name,\n",
        "                                          device=self.hf_device_id)\n",
        "                    self.translation_pipelines[lang_code] = translator\n",
        "                    print(f\"    翻譯模型 for zh -> {lang_code} ({model_name}) 載入成功。\")\n",
        "                    loaded_any_model = True\n",
        "                except Exception as e:\n",
        "                    print(f\"    載入翻譯模型 for zh -> {lang_code} ({model_name}) 失敗: {e}\")\n",
        "                    self.translation_pipelines[lang_code] = None\n",
        "            else:\n",
        "                print(f\"  未找到針對 zh -> {lang_code} 的預定義翻譯模型。\")\n",
        "        if not loaded_any_model:\n",
        "            print(\"警告: 未能成功載入任何Hugging Face翻譯模型。自動翻譯功能將受限。\")\n",
        "        return self.translation_pipelines\n",
        "\n",
        "    def cleanup(self):\n",
        "        print(\"正在清理模型資源...\")\n",
        "        if self.clip_model: del self.clip_model\n",
        "        if self.clip_preprocess: del self.clip_preprocess\n",
        "        if self.sd_pipeline: del self.sd_pipeline\n",
        "        if self.translation_pipelines:\n",
        "            for lang, pipe in self.translation_pipelines.items():\n",
        "                if pipe: del pipe\n",
        "            self.translation_pipelines.clear()\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"  模型和資源清理操作已執行。\")"
      ],
      "metadata": {
        "id": "WGwczTjNvDFN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#用來處理:詞彙概念的建立與管理，\"希望能\"幫我把一個「中文概念」翻譯成多種語言版本（中、英、日、韓）\n",
        "#並且記錄每個單字、原始中文、和各語言的詞彙說明(這部分一直做不太成功)\n",
        "#總之就是自動翻譯，也請ai幫我做過好幾版，結果都不太好，又經過好幾次自己手動改跟摸索......\n",
        "\n",
        "class ConceptDataProvider:\n",
        "    def __init__(self, clear_default_concepts=False): # 新增參數\n",
        "        self.word_concepts_list = [] # 默認清空，由用戶在main中添加\n",
        "        if not clear_default_concepts:\n",
        "            # 如果需要，可以保留原有的預設列表\n",
        "            self.word_concepts_list = [\n",
        "                {\"concept_name\": \"cool_ambiguous\", \"base_chinese\": \"酷 / 涼爽\", \"translations\": {\"zh\": \"冰涼的飲料，酷炫的風格，冷靜的態度\", \"en\": \"cool refreshing drink, cool stylish look, calm and cool attitude\", \"ja\": \"冷たい飲み物、かっこいいスタイル、冷静な態度\", \"ko\": \"시원한 음료, 멋진 스타일, 침착한 태도\"}},\n",
        "\n",
        "            ]\n",
        "        self.word_concepts_to_process = self.word_concepts_list\n",
        "        print(f\"8:初始定義了 {len(self.word_concepts_list)} 個詞彙概念。\")\n",
        "\n",
        "    def _generate_concept_name(self, base_chinese):\n",
        "        \"\"\"根據中文詞彙生成一個簡化的英文概念名\"\"\"\n",
        "        name = re.sub(r'[^\\w]', '', base_chinese) #移除非字母數字字符(測試中總是亂碼)\n",
        "        name = name[:20] #限制長度\n",
        "        if not name: name = \"unnamed_concept\"\n",
        "        return f\"{name}_auto_translated\"\n",
        "\n",
        "    #自動呼叫翻譯模型，核心功能~~~\n",
        "    def add_concept_with_auto_translation(self, translation_pipelines, base_chinese,\n",
        "                                          chinese_description_for_prompt=None, target_languages=['en', 'ja', 'ko']):\n",
        "        \"\"\"\n",
        "        新增一個概念，並使用 Hugging Face pipeline 自動翻譯其描述性句子。\n",
        "        如果 chinese_description_for_prompt 未提供，則直接翻譯 base_chinese。\n",
        "        \"\"\"\n",
        "        concept_name = self._generate_concept_name(base_chinese)\n",
        "        text_to_translate = chinese_description_for_prompt if chinese_description_for_prompt else base_chinese\n",
        "\n",
        "        print(f\"\\n  準備新增概念 '{concept_name}' (基於 '{base_chinese}')...\")\n",
        "        print(f\"    將翻譯: '{text_to_translate[:50]}...'\")\n",
        "\n",
        "        if not translation_pipelines or not any(translation_pipelines.values()):\n",
        "            print(\"    錯誤：翻譯 pipelines 未提供或均未成功載入，無法自動翻譯。將使用原文作為提示。\")\n",
        "            translations = {lang: text_to_translate for lang in target_languages}\n",
        "            translations['zh'] = text_to_translate\n",
        "        else:\n",
        "            translations = {\"zh\": text_to_translate}\n",
        "            for lang_code in target_languages:\n",
        "                translator_pipeline = translation_pipelines.get(lang_code)\n",
        "                if translator_pipeline:\n",
        "                    try:\n",
        "                        translated_result = translator_pipeline(text_to_translate)\n",
        "                        translated_text = translated_result[0]['translation_text']\n",
        "                        translations[lang_code] = translated_text\n",
        "                        print(f\"      -> {lang_code.upper()}: {translated_text}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"      使用Hugging Face模型翻譯到 {lang_code.upper()} 失敗: {e}\")\n",
        "\n",
        "                        #原本的AI生成\n",
        "                        # translations[lang_code] = f\"翻譯失敗: {text_to_translate}\"\n",
        "                        #更改\n",
        "                        translations[lang_code] = text_to_translate\n",
        "                #原本的AI生成\n",
        "                # else:\n",
        "                #     print(f\"      未找到 {lang_code.upper()} 的翻譯 pipeline，使用原文。\")\n",
        "                #     translations[lang_code] = text_to_translate\n",
        "                #更改\n",
        "                else:\n",
        "                      print(f\"未找到{lang_code.upper()}的翻譯pipeline，該語言將跳過。\")\n",
        "                #不加入translations，後面自動對不存在語言略過\n",
        "\n",
        "        new_concept = {\n",
        "            \"concept_name\": concept_name,\n",
        "            \"base_chinese\": base_chinese,\n",
        "            \"translations\": translations\n",
        "        }\n",
        "        self.word_concepts_list.append(new_concept)\n",
        "        print(f\"  新概念 '{concept_name}' 已成功添加。現有 {len(self.word_concepts_list)} 個概念。\")\n",
        "        print(f\"8(更新):定義了 {len(self.word_concepts_to_process)} 個詞彙概念用於本次分析。\")\n",
        "\n",
        "    def get_concepts_to_process(self):\n",
        "        if not self.word_concepts_to_process:\n",
        "            print(\"警告：沒有定義任何詞彙概念進行處理。\")\n",
        "        return self.word_concepts_to_process"
      ],
      "metadata": {
        "id": "w4a_ashAvGdl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#主要處理CLIP的文本分析功能\n",
        "#作用1.把多語言的提示詞送進CLIP模型，轉換成向量\n",
        "#作用2.比較不同語言之間的語意相似度\n",
        "#希望能以此判斷「翻譯後的提示詞」在語意上是不是相近\n",
        "#這段原本也是ai生成的，經過我的理解與整理:\n",
        "\n",
        "class TextAnalyzer:\n",
        "    def __init__(self, clip_model, device):\n",
        "        self.clip_model = clip_model\n",
        "        self.device = device\n",
        "    def get_clip_text_embeddings(self, text_prompts_dict):\n",
        "        if self.clip_model is None:\n",
        "            print(\"  警告: CLIP 模型未載入，文本嵌入將為零向量。\")\n",
        "            return {lang_code: np.zeros(512, dtype=np.float32) for lang_code in text_prompts_dict}\n",
        "        text_embeddings_result_dict = {}\n",
        "        with torch.no_grad():\n",
        "            for lang_tag, text_content in text_prompts_dict.items():\n",
        "                try:\n",
        "                    tokenized_input_text = clip.tokenize([text_content]).to(self.device)\n",
        "                    text_semantic_features = self.clip_model.encode_text(tokenized_input_text)\n",
        "                    text_semantic_features /= text_semantic_features.norm(dim=-1, keepdim=True)\n",
        "                    text_embeddings_result_dict[lang_tag] = text_semantic_features.cpu().numpy().flatten()\n",
        "                except Exception as e:\n",
        "                    print(f\"為 '{lang_tag}':'{text_content[:30]}...' 生成CLIP嵌入時出錯: {e}\")\n",
        "                    text_embeddings_result_dict[lang_tag] = np.zeros(512, dtype=np.float32)\n",
        "        return text_embeddings_result_dict\n",
        "    def calculate_embedding_similarity(self, embeddings_dict, reference_lang='en'):\n",
        "        similarity_scores_result = {}\n",
        "        if reference_lang not in embeddings_dict or embeddings_dict.get(reference_lang) is None or np.all(np.isclose(embeddings_dict[reference_lang], 0)):\n",
        "            print(f\"  參考語言 '{reference_lang.upper()}' 的嵌入向量無效或不存在，無法計算相似度。\")\n",
        "            return {f\"{reference_lang}_vs_{lang}\": None for lang in embeddings_dict if lang != reference_lang}\n",
        "        print(f\"  CLIP文本嵌入向量餘弦相似度 (vs '{reference_lang.upper()}'):\")\n",
        "        ref_embedding = embeddings_dict[reference_lang].reshape(1, -1)\n",
        "        for lang, emb in embeddings_dict.items():\n",
        "            if lang == reference_lang: continue\n",
        "            sim_val_str = \"N/A (嵌入無效)\"\n",
        "            sim_num = None\n",
        "            if emb is not None and not np.all(np.isclose(emb, 0)):\n",
        "                sim_num = cosine_similarity(ref_embedding, emb.reshape(1, -1))[0][0]\n",
        "                sim_val_str = f\"{sim_num:.3f}\"\n",
        "            similarity_scores_result[f\"{reference_lang}_vs_{lang}\"] = sim_num\n",
        "            print(f\"    - 與 {lang.upper()}: {sim_val_str}\")\n",
        "        return similarity_scores_result"
      ],
      "metadata": {
        "id": "Dh5nle1GvHpl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#處理「圖像生成與分析」的功能\n",
        "#作用1.用Stable Diffusion模型根據提示詞生成圖片(目前成效不佳)\n",
        "#作用2.圖片中提取出主色\n",
        "#作用3.分析圖片的整體亮度、對比、飽和度\n",
        "\n",
        "class ImageProcessor:\n",
        "    def __init__(self, sd_pipeline, device):\n",
        "        self.sd_pipeline = sd_pipeline\n",
        "        self.device = device\n",
        "\n",
        "    #測試\n",
        "    def generate_image_from_prompt(self, prompt):\n",
        "        image = self.sd_pipeline(prompt).images[0]\n",
        "        return image\n",
        "\n",
        "\n",
        "    def generate_image_with_sd(self, prompt_text, random_seed=42, inference_steps=30, cfg_scale=7.5):\n",
        "        if self.sd_pipeline is None:\n",
        "            placeholder_img = Image.new('RGB', (512, 512), color='silver')\n",
        "            draw = ImageDraw.Draw(placeholder_img)\n",
        "            try: font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n",
        "            except IOError: font = ImageFont.load_default()\n",
        "            draw.text((10, 10), f\"SD模型未載入\\n提示:\\n{prompt_text[:70]}...\", fill=(60, 60, 60), font=font)\n",
        "            return placeholder_img\n",
        "        try:\n",
        "            gen = torch.Generator(device=self.device).manual_seed(random_seed)\n",
        "            with torch.no_grad():\n",
        "                img = self.sd_pipeline(prompt_text, num_inference_steps=inference_steps, guidance_scale=cfg_scale, generator=gen).images[0]\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"  生成圖像時出錯 ('{prompt_text[:40]}...'): {e}\")\n",
        "            error_img = Image.new('RGB', (512, 512), color='lightcoral')\n",
        "            draw = ImageDraw.Draw(error_img)\n",
        "            try: font = ImageFont.truetype(\"DejaVuSans.ttf\", 15)\n",
        "            except IOError: font = ImageFont.load_default()\n",
        "            draw.text((10, 10), f\"圖像生成錯誤:\\n{prompt_text[:60]}...\\n錯誤:\\n{str(e)[:100]}\", fill=(0, 0, 0), font=font)\n",
        "            return error_img\n",
        "    def extract_dominant_colors(self, pil_img, num_colors=5):\n",
        "\n",
        "        #原本的AI生成\n",
        "        # if pil_img is None or (hasattr(pil_img, 'width') and pil_img.width < num_colors) or \\\n",
        "        #    (hasattr(pil_img, 'height') and pil_img.height < num_colors):\n",
        "\n",
        "        #更改\n",
        "        if pil_img is None or pil_img.width * pil_img.height < num_colors * 4:\n",
        "             rgb_fallback = np.array([[128, 128, 128]] * num_colors, dtype=int)\n",
        "\n",
        "             rgb_fallback = np.array([[128, 128, 128]] * num_colors, dtype=int)\n",
        "             lab_fallback = rgb2lab(rgb_fallback.reshape((num_colors, 1, 3)) / 255.0).reshape((num_colors, 3))\n",
        "             return rgb_fallback, lab_fallback\n",
        "        try:\n",
        "            img_rgb = pil_img.convert('RGB')\n",
        "            max_dim = 150\n",
        "            ratio = max_dim / max(img_rgb.width, img_rgb.height)\n",
        "            new_size = (max(1, int(img_rgb.width * ratio)), max(1, int(img_rgb.height * ratio)))\n",
        "            img_res = img_rgb.resize(new_size, Image.Resampling.LANCZOS)\n",
        "            pixels = np.array(img_res).reshape(-1, 3)\n",
        "            if pixels.shape[0] < num_colors:\n",
        "                rgb_colors = np.zeros((num_colors, 3), dtype=int)\n",
        "                actual_extracted_colors = pixels.astype(int)\n",
        "                rgb_colors[:actual_extracted_colors.shape[0]] = actual_extracted_colors\n",
        "                if actual_extracted_colors.shape[0] < num_colors:\n",
        "                    rgb_colors[actual_extracted_colors.shape[0]:] = np.array([128,128,128])\n",
        "            else:\n",
        "                kmeans = KMeans(n_clusters=num_colors, random_state=0, n_init='auto', max_iter=200).fit(pixels)\n",
        "                rgb_colors = kmeans.cluster_centers_.astype(int)\n",
        "            if rgb_colors.shape[0] < num_colors:\n",
        "                padded_colors = np.full((num_colors, 3), 128, dtype=int)\n",
        "                padded_colors[:rgb_colors.shape[0]] = rgb_colors\n",
        "                rgb_colors = padded_colors\n",
        "            lab_colors = rgb2lab(rgb_colors.reshape((num_colors, 1, 3)) / 255.0).reshape((num_colors, 3))\n",
        "            return rgb_colors, lab_colors\n",
        "        except Exception as e:\n",
        "            print(f\"  提取主色調時出錯: {e}\")\n",
        "            rgb_err = np.array([[100, 100, 100]] * num_colors, dtype=int)\n",
        "            lab_err = rgb2lab(rgb_err.reshape((num_colors,1,3)) / 255.0).reshape((num_colors, 3))\n",
        "            return rgb_err, lab_err\n",
        "    def analyze_global_features(self, pil_image):\n",
        "        if pil_image is None: return {\"avg_brightness\": \"N/A\", \"contrast_std\": \"N/A\", \"avg_saturation\": \"N/A\"}\n",
        "        try:\n",
        "            cv_bgr = np.array(pil_image.convert('RGB'))[:, :, ::-1].copy()\n",
        "            gray = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2GRAY)\n",
        "            brightness = round(np.mean(gray), 2)\n",
        "            contrast = round(np.std(gray), 2)\n",
        "            hsv = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2HSV)\n",
        "            saturation = round(np.mean(hsv[:, :, 1]), 2)\n",
        "            return {\"avg_brightness\": brightness, \"contrast_std\": contrast, \"avg_saturation\": saturation}\n",
        "        except Exception as e:\n",
        "            print(f\"  分析全局圖像特徵時出錯: {e}\")\n",
        "            return {\"avg_brightness\": \"Err\", \"contrast_std\": \"Err\", \"avg_saturation\": \"Err\"}"
      ],
      "metadata": {
        "id": "v1Dmo3XwvI8d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#分析報告\n",
        "#作用1.根據概念和分析結果，產出文字說明框架\n",
        "#作用2.把圖片、顏色、CLIP相似度、特徵等等畫成一張完整比較圖\n",
        "#以上就是文字理解~圖像生成~結果分析~報告輸出\n",
        "\n",
        "class ReportGenerator:\n",
        "    def generate_explanation_template(self, chinese_concept, lang_prompt_info, dom_colors_hex=None, global_feats=None):\n",
        "        expl = f\"\\n--- 解釋模板 for 概念:【{chinese_concept}】| 語言提示: 【{lang_prompt_info[:70]}...】 ---\\n\"\n",
        "        if dom_colors_hex: expl += f\"圖像主要色票 (HEX): {', '.join(dom_colors_hex[:3])} ...\\n\"\n",
        "        if global_feats:\n",
        "            expl += f\"全局圖像特徵: 亮度={global_feats.get('avg_brightness', 'N/A')}, \"\n",
        "            expl += f\"對比度={global_feats.get('contrast_std', 'N/A')}, 飽和度={global_feats.get('avg_saturation', 'N/A')}\\n\"\n",
        "        expl += f\"\\n原因推測與圖像描述 (請您填充)：\\n\"\n",
        "        expl += f\"   [請結合以上客觀指標和您的觀察，詳細闡述：\\n\"\n",
        "        expl += f\"    a. 圖像視覺風格與氛圍？\\n    b. 主要元素與提示詞的關聯？\\n    c. 色彩運用如何詮釋提示詞？\\n\"\n",
        "        expl += f\"    d. (特定語言)文化背景的可能影響？\\n    e. 與其他語言生成圖像的差異及可能原因？]\\n\"\n",
        "        expl += f\"--------------------------------------------------------------------------\\n\"\n",
        "        return expl\n",
        "    def plot_concept_results(self, concept_id, base_chinese, prompts_dict, images_dict,colors_dict, similarities_dict, global_features_dict=None):\n",
        "        langs = list(prompts_dict.keys())\n",
        "        num_langs = len(langs)\n",
        "        if num_langs == 0:\n",
        "            print(f\"概念 '{concept_id}' 無 prompts 可繪製。\")\n",
        "            return\n",
        "        h_ratio, w_ratio = 2.8, 4.0\n",
        "        total_h, total_w = h_ratio * 2, w_ratio * num_langs if num_langs > 0 else w_ratio\n",
        "        fig, axs = plt.subplots(2, max(1, num_langs), figsize=(total_w, total_h), gridspec_kw={'height_ratios': [0.78, 0.22]})\n",
        "        if num_langs == 1: axs = axs.reshape(2, 1)\n",
        "\n",
        "        title_base = f\"概念分析: '{base_chinese}' ({concept_id})\\nCLIP相似度(vs EN): \"\n",
        "        sim_strs = []\n",
        "        if similarities_dict:\n",
        "            sim_strs = [f\"{k.split('_vs_')[-1].upper()}: {v:.2f}\" if isinstance(v, (float, np.floating)) else f\"{k.split('_vs_')[-1].upper()}: {v}\"\n",
        "                        for k, v in similarities_dict.items()]\n",
        "        fig.suptitle(title_base + \", \".join(sim_strs), fontsize=11, y=1.04)\n",
        "        for i, lang in enumerate(langs):\n",
        "            img = images_dict.get(lang)\n",
        "            colors_data = colors_dict.get(lang)\n",
        "            global_feats_this_lang = (global_features_dict or {}).get(lang, {})\n",
        "            ax_img = axs[0, i]; ax_color = axs[1, i]\n",
        "            if img: ax_img.imshow(img)\n",
        "            else: ax_img.text(0.5, 0.5, '圖像未生成', ha='center', va='center', transform=ax_img.transAxes)\n",
        "            img_title_prompt = prompts_dict.get(lang, \"N/A\")\n",
        "            img_title = f\"{lang.upper()}: \\\"{img_title_prompt[:30]}\\\"...\"\n",
        "            if global_feats_this_lang:\n",
        "                img_title += f\"\\n亮:{global_feats_this_lang.get('avg_brightness', '-')} 對比:{global_feats_this_lang.get('contrast_std', '-')} 飽:{global_feats_this_lang.get('avg_saturation', '-')}\"\n",
        "            ax_img.set_title(img_title, fontsize=7.5); ax_img.axis('off')\n",
        "            if colors_data:\n",
        "                rgb_patch, lab_patch = colors_data\n",
        "                if rgb_patch is not None and len(rgb_patch) > 0 :\n",
        "                    n_patch = len(rgb_patch)\n",
        "                    patch_canvas = np.zeros((25, 100, 3), dtype=np.uint8)\n",
        "                    patch_w = 100 // n_patch\n",
        "                    for j, rgb_c in enumerate(rgb_patch): patch_canvas[:, j * patch_w:(j + 1) * patch_w] = rgb_c\n",
        "                    ax_color.imshow(patch_canvas)\n",
        "                    lab_str_parts = []\n",
        "                    if lab_patch is not None:\n",
        "                        for l_val, a_val, b_val in lab_patch[:min(3,n_patch)]: lab_str_parts.append(f\"L{l_val:.0f} a{a_val:.0f} b{b_val:.0f}\")\n",
        "                        lab_str = \"\\n\".join(lab_str_parts)\n",
        "                        ax_color.set_title(f\"Lab(Top{min(3,n_patch)}):\\n{lab_str}\", fontsize=6)\n",
        "                    else: ax_color.set_title(f\"RGB顏色\", fontsize=6)\n",
        "                else: ax_color.text(0.5,0.5,'無顏色數據',ha='center',va='center',transform=ax_color.transAxes, fontsize=6)\n",
        "            else: ax_color.text(0.5, 0.5, '無顏色', ha='center', va='center', transform=ax_color.transAxes, fontsize=6)\n",
        "            ax_color.axis('off')\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.94]); plt.subplots_adjust(hspace=0.5, wspace=0.3); plt.show()"
      ],
      "metadata": {
        "id": "NGGBRHnJvKVl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#串接所有模組(文字分析+圖像處理+報告生成)\n",
        "#除錯誤加了if prompt is None:#遇到翻譯缺失就跳\n",
        "\n",
        "class AnalysisPipeline:\n",
        "    def __init__(self, config_manager, model_manager, concept_provider, text_analyzer, image_processor, report_generator):\n",
        "        self.config = config_manager\n",
        "        self.models = model_manager\n",
        "        self.concepts_provider = concept_provider\n",
        "        self.text_analyzer = text_analyzer\n",
        "        self.image_processor = image_processor\n",
        "        self.reporter = report_generator\n",
        "        self.base_seed = 20240101\n",
        "        self.sd_steps = 20\n",
        "        self.sd_cfg = 7.0  # 減少步數以加速\n",
        "        self.num_dom_colors = 5\n",
        "        self.save_images_flag = True\n",
        "\n",
        "    def run_analysis(self):\n",
        "        print(f\"Cell 15: 即將開始執行主流程...\")\n",
        "        concepts_to_process = self.concepts_provider.get_concepts_to_process()\n",
        "        if not concepts_to_process:\n",
        "            print(\"沒有概念需要處理，流程結束。\")\n",
        "            return []\n",
        "        print(f\"  將處理 {len(concepts_to_process)} 個詞彙概念...\")\n",
        "        results_collection = []\n",
        "        for concept_idx, concept_detail in enumerate(tqdm(concepts_to_process, desc=\"總體概念處理\")):\n",
        "            concept_id = concept_detail[\"concept_name\"]\n",
        "            base_zh = concept_detail[\"base_chinese\"]\n",
        "            prompts = concept_detail[\"translations\"]\n",
        "            print(f\"\\n\\n處理概念 #{concept_idx + 1}: '{base_zh}' ({concept_id})\")\n",
        "            print(\"  [1. CLIP嵌入分析]\")\n",
        "            embeddings = self.text_analyzer.get_clip_text_embeddings(prompts)\n",
        "            similarities = self.text_analyzer.calculate_embedding_similarity(embeddings, reference_lang='en')\n",
        "            print(\"  [2. 圖像生成、顏色與全局特徵分析]\")\n",
        "            concept_images = {}\n",
        "            concept_colors = {}\n",
        "            concept_global_features = {}\n",
        "            concept_explanations_str = \"\"\n",
        "\n",
        "            for lang_idx, (lang, prompt) in enumerate(tqdm(prompts.items(), desc=f\"  '{concept_id}'語言處理\", leave=False)):\n",
        "                if not prompt or not prompt.strip():  # 遇到翻譯缺失或空白就跳過\n",
        "                    print(f\"    -> {lang.upper()}: ⚠️ 翻譯缺失或空白，跳過。\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"    -> {lang.upper()}: '{prompt}'\")\n",
        "                img_seed = self.base_seed + concept_idx * 100 + lang_idx * 10\n",
        "\n",
        "                pil_img = self.image_processor.generate_image_with_sd(\n",
        "                    prompt,\n",
        "                    random_seed=img_seed,\n",
        "                    inference_steps=self.sd_steps,\n",
        "                    cfg_scale=self.sd_cfg\n",
        "                )\n",
        "                concept_images[lang] = pil_img\n",
        "\n",
        "                if self.save_images_flag and pil_img:\n",
        "                    try:\n",
        "                        fname = f\"{concept_id}_{lang}_s{img_seed}.png\"\n",
        "                        fpath = os.path.join(self.config.images_out_dir, fname)\n",
        "                        pil_img.save(fpath)\n",
        "                    except Exception as e:\n",
        "                        print(f\"      儲存圖像'{fname}'失敗: {e}\")\n",
        "\n",
        "                rgb_cs, lab_cs = self.image_processor.extract_dominant_colors(pil_img, self.num_dom_colors)\n",
        "                concept_colors[lang] = (rgb_cs, lab_cs)\n",
        "\n",
        "                global_feats = self.image_processor.analyze_global_features(pil_img)\n",
        "                concept_global_features[lang] = global_feats\n",
        "\n",
        "                hex_colors = [f\"#{c[0]:02x}{c[1]:02x}{c[2]:02x}\" for c in rgb_cs] if rgb_cs and len(rgb_cs) > 0 else []\n",
        "\n",
        "                expl_text = self.reporter.generate_explanation_template(\n",
        "                    base_zh,\n",
        "                    f\"{lang.upper()}: {prompt}\",\n",
        "                    hex_colors,\n",
        "                    global_feats\n",
        "                )\n",
        "                print(expl_text)\n",
        "                concept_explanations_str += expl_text\n",
        "\n",
        "                if self.config.device == \"cuda\":\n",
        "                    torch.cuda.empty_cache()\n",
        "                    time.sleep(0.05)\n",
        "\n",
        "            print(\"\\n  [3. 繪製結果圖表]\")\n",
        "            self.reporter.plot_concept_results(\n",
        "                concept_id,\n",
        "                base_zh,\n",
        "                prompts,\n",
        "                concept_images,\n",
        "                concept_colors,\n",
        "                similarities,\n",
        "                concept_global_features\n",
        "            )\n",
        "            results_collection.append({\n",
        "                \"concept\": concept_id,\n",
        "                \"base_chinese\": base_zh,\n",
        "                \"prompts\": prompts,\n",
        "                \"similarities\": similarities,\n",
        "                \"global_features\": concept_global_features,\n",
        "                \"explanation_prompts_combined\": concept_explanations_str\n",
        "            })\n",
        "            print(f\"  概念 '{concept_id}' 分析完畢。\")\n",
        "\n",
        "            if self.config.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        print(\"\\n\\n所有詞彙概念處理完成！解釋模板已在上方打印。\")\n",
        "        return results_collection\n"
      ],
      "metadata": {
        "id": "jOp8Pt8zvLg9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.執行分析流程或跳過\n",
        "concept_provider = ConceptDataProvider(clear_default_concepts=True)\n",
        "text_analyzer = TextAnalyzer(clip_model=clip_model, device=config_mgr.device)\n",
        "image_processor = ImageProcessor(sd_pipeline=sd_pipeline, device=config_mgr.device)\n",
        "report_generator = ReportGenerator()\n",
        "\n",
        "if not concept_provider.get_concepts_to_process():\n",
        "    print(\"沒有任何概念被定義，無法執行分析流程。請檢查 `chinese_terms_to_analyze`。\")\n",
        "else:\n",
        "    # 使用修正後的分析流程（避免 numpy 陣列布林判斷錯誤）\n",
        "    def safe_run_analysis():\n",
        "        results = []\n",
        "        concepts = concept_provider.get_concepts_to_process()\n",
        "\n",
        "        for concept in concepts:\n",
        "            print(f\"\\n🔍 分析概念: {concept['base_chinese']} ({concept['translations']})\")\n",
        "            concept_global_features = {}\n",
        "\n",
        "            # 從 translations 生成提示詞\n",
        "            prompts = {}\n",
        "            for lang, translated_desc in concept['translations'].items():\n",
        "                prompts[lang] = translated_desc  # 假設 translation 文字本身就是提示用語句\n",
        "\n",
        "            for lang, prompt in prompts.items():\n",
        "                print(f\"  ➤ 處理語言: {lang} | 提示詞: {prompt}\")\n",
        "\n",
        "                # 1. 產生圖片\n",
        "                image = image_processor.generate_image_from_prompt(prompt)\n",
        "\n",
        "                # 2. 分析圖像特徵\n",
        "                global_feats, rgb_cs = text_analyzer.extract_features_and_colors(image)\n",
        "                concept_global_features[lang] = global_feats\n",
        "\n",
        "                if rgb_cs is not None and len(rgb_cs) > 0:\n",
        "                    hex_colors = [f\"#{c[0]:02x}{c[1]:02x}{c[2]:02x}\" for c in rgb_cs]\n",
        "                else:\n",
        "                    hex_colors = []\n",
        "\n",
        "                # 3. 生成解釋文字\n",
        "                expl_text = report_generator.generate_explanation_template(\n",
        "                    base_chinese=concept['base_chinese'],\n",
        "                    prompt_lang=lang,\n",
        "                    prompt_text=prompt,\n",
        "                    hex_colors=hex_colors\n",
        "                )\n",
        "\n",
        "                # 4. 彙總結果\n",
        "                result_entry = {\n",
        "                    'concept': concept['base_chinese'],\n",
        "                    'language': lang,\n",
        "                    'prompt': prompt,\n",
        "                    'colors': hex_colors,\n",
        "                    'features': global_feats,\n",
        "                    'explanation': expl_text,\n",
        "                    'image': image\n",
        "                }\n",
        "\n",
        "                results.append(result_entry)\n",
        "\n",
        "        return results\n",
        "\n",
        "    all_results = safe_run_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kzu8lAKtYMl",
        "outputId": "c5b1a65d-fba8-49c7-8f16-aa9f15d57ee0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8:初始定義了 0 個詞彙概念。\n",
            "警告：沒有定義任何詞彙概念進行處理。\n",
            "沒有任何概念被定義，無法執行分析流程。請檢查 `chinese_terms_to_analyze`。\n"
          ]
        }
      ]
    }
  ]
}