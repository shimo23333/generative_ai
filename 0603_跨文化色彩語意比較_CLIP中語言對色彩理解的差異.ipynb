{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shimo23333/generative_ai/blob/main/0603_%E8%B7%A8%E6%96%87%E5%8C%96%E8%89%B2%E5%BD%A9%E8%AA%9E%E6%84%8F%E6%AF%94%E8%BC%83_CLIP%E4%B8%AD%E8%AA%9E%E8%A8%80%E5%B0%8D%E8%89%B2%E5%BD%A9%E7%90%86%E8%A7%A3%E7%9A%84%E5%B7%AE%E7%95%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b5MppSXtAZe",
        "outputId": "7915d720-bebf-4f20-c897-51b53766f322",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  fonts-noto-cjk-extra\n",
            "The following NEW packages will be installed:\n",
            "  fonts-noto-cjk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 61.2 MB of archives.\n",
            "After this operation, 93.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-cjk all 1:20220127+repack1-1 [61.2 MB]\n",
            "Fetched 61.2 MB in 2s (31.7 MB/s)\n",
            "Selecting previously unselected package fonts-noto-cjk.\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-noto-cjk_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Setting up fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ],
      "source": [
        "#1:å®‰è£\n",
        "!pip install -q git+https://github.com/openai/CLIP.git #å¾GitHubå®‰è£OpenAIçš„CLIP\n",
        "!pip install -q scikit-image #è™•ç†å½±åƒ\n",
        "!pip install -q opencv-python #å½±åƒè™•ç†å·¥å…·(OpenCV)\n",
        "!pip install -q diffusers transformers accelerate invisible-watermark safetensors #ç‚ºäº†ä½¿ç”¨StableDiffusion\n",
        "!pip install -q ipywidgets\n",
        "!pip install -q sentencepiece sacremoses #ç¿»è­¯æ¨¡å‹\n",
        "!apt-get -y install fonts-noto-cjk #å˜—è©¦"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2:åŒ¯å…¥\n",
        "import torch\n",
        "import clip #è½‰æˆå‘é‡\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans #ç”¨ä¾†æ‰¾åœ–ç‰‡è£¡æœ€ä¸»è¦çš„å¹¾å€‹é¡è‰²\n",
        "from skimage.color import rgb2lab #æŠŠåœ–ç‰‡å¾ RGB é¡è‰²è½‰æˆ LAB é¡è‰²ç©ºé–“çš„å·¥å…·\n",
        "import cv2\n",
        "from diffusers import StableDiffusionPipeline #tableDiffusion\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity #æ¯”æ–‡å­—èˆ‡åœ–åƒçš„èªæ„æ˜¯ä¸æ˜¯æ¥è¿‘?\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import matplotlib.font_manager as fm\n",
        "from transformers import pipeline\n",
        "import re #ç”Ÿæˆconcept_name\n",
        "\n",
        "print(\"2:å‡½å¼åº«åŒ¯å…¥å®Œæˆã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsQlITEMtKou",
        "outputId": "f1efe2bc-e704-4bd8-ad1d-ea3da24593fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2:å‡½å¼åº«åŒ¯å…¥å®Œæˆã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#é€™æ˜¯ä¸€å€‹ç”¨ä¾†è¨­å®šã€Œæ•´å€‹å°ˆæ¡ˆç’°å¢ƒã€çš„é¡åˆ¥ï¼Œè‡ªå‹•æª¢æŸ¥æ˜¯ä¸æ˜¯èƒ½ç”¨GPUã€è¨­å®šå¥½ç•«åœ–ç”¨çš„ä¸­æ–‡å­—é«”ï¼ŒåŒæ™‚ç¢ºä¿åœ–åƒè¦å­˜çš„è³‡æ–™å¤¾å·²ç¶“å»ºç«‹å¥½ã€‚\n",
        "#åŸæœ¬æ˜¯aiç”Ÿæˆçš„ä½†å·²ç¶“éæˆ‘çš„ç†è§£èˆ‡æ•´ç†\n",
        "class ConfigManager:\n",
        "    def __init__(self, font_path_cjk='/usr/share/fonts/opentype/noto/NotoSansCJKjp-Regular.otf',\n",
        "                 images_out_dir=\"project_outputs_final_v4_oop_hf\"):\n",
        "        self.device = self._get_device()\n",
        "        self.font_path_cjk = font_path_cjk\n",
        "        self.images_out_dir = images_out_dir\n",
        "        self._setup_matplotlib_font()\n",
        "        self._setup_output_directory()\n",
        "        self.hf_device_id = 0 if self.device == \"cuda\" else -1\n",
        "\n",
        "    def _get_device(self):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"4:æœ¬æ¬¡é‹è¡Œçš„è¨ˆç®—è¨­å‚™æ˜¯: {device}\")\n",
        "        if device == \"cpu\":\n",
        "            print(\"è­¦å‘Šï¼šæœªä½¿ç”¨GPUï¼é‹è¡Œå¤§å‹AIæ¨¡å‹æœƒéå¸¸æ…¢ã€‚\")\n",
        "        return device\n",
        "\n",
        "    def _setup_matplotlib_font(self):\n",
        "        print(\"3:æ­£åœ¨è¨­å®š Matplotlib CJK å­—é«”...\")\n",
        "        if os.path.exists(self.font_path_cjk):\n",
        "            try:\n",
        "                fm.fontManager.addfont(self.font_path_cjk)\n",
        "                prop = fm.FontProperties(fname=self.font_path_cjk)\n",
        "                font_name = prop.get_name()\n",
        "                plt.rcParams['font.family'] = font_name\n",
        "                plt.rcParams['axes.unicode_minus'] = False\n",
        "                print(f\"  å·²æˆåŠŸè¨­å®š CJK å­—é«”ç‚º: {font_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  è¨­å®šå­—é«” '{self.font_path_cjk}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}. é€€åˆ°é€šç”¨åˆ—è¡¨ã€‚\")\n",
        "                plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'SimHei', 'DejaVu Sans', 'sans-serif']\n",
        "        else:\n",
        "            print(f\"  æŒ‡å®šçš„ CJK å­—é«”æ–‡ä»¶è·¯å¾‘ä¸å­˜åœ¨: {self.font_path_cjk}. é€€åˆ°é€šç”¨åˆ—è¡¨ã€‚\")\n",
        "            plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'SimHei', 'DejaVu Sans', 'sans-serif']\n",
        "        plt.rcParams['axes.unicode_minus'] = False\n",
        "        print(\"3:Matplotlib CJK å­—é«”è¨­å®šå®Œç•¢ã€‚\")\n",
        "\n",
        "    def _setup_output_directory(self):\n",
        "        if not os.path.exists(self.images_out_dir):\n",
        "            os.makedirs(self.images_out_dir)\n",
        "            print(f\"  å·²å»ºç«‹åœ–åƒå„²å­˜ç›®éŒ„: {self.images_out_dir}\")"
      ],
      "metadata": {
        "id": "Iv48bKSCvB41"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#é€™å€‹é¡åˆ¥æ˜¯ç”¨ä¾†è™•ç†ã€Œå„ç¨®æ¨¡å‹çš„è¼‰å…¥èˆ‡ç®¡ç†ã€çš„ï¼Œå¹«æˆ‘æŠŠä¸‰ç¨®é‡è¦çš„æ¨¡å‹æº–å‚™å¥½ï¼ŒCLIPã€Stable Diffusionã€ç¿»è­¯æ¨¡å‹\n",
        "#åŸæœ¬æ˜¯aiç”Ÿæˆçš„ä½†å·²ç¶“éæˆ‘çš„ç†è§£èˆ‡æ•´ç†ï¼Œä¸¦ä¸”ä¹Ÿè‡ªå·±æ‰¾äº†æ‡‰è©²å¯è¡Œçš„æ¨¡å‹\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, device, hf_device_id):\n",
        "        self.device = device\n",
        "        self.hf_device_id = hf_device_id\n",
        "        self.clip_model = None\n",
        "        self.clip_preprocess = None\n",
        "        self.sd_pipeline = None\n",
        "        self.translation_pipelines = {}\n",
        "\n",
        "    def load_clip_model(self, model_name=\"ViT-B/32\"):\n",
        "        print(f\"5:æº–å‚™è¼‰å…¥ CLIP æ¨¡å‹ ({model_name})...\")\n",
        "        if self.device == \"cuda\":\n",
        "            try:\n",
        "                self.clip_model, self.clip_preprocess = clip.load(model_name, device=self.device)\n",
        "                self.clip_model.eval()\n",
        "                print(f\"  CLIP æ¨¡å‹ ({model_name}) å·²æˆåŠŸè¼‰å…¥åˆ° {self.device}ï¼\")\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                print(f\"  è¼‰å…¥CLIPæ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "                self.clip_model, self.clip_preprocess = None, None\n",
        "        else:\n",
        "            print(f\"  æœªå¯¦éš›è¼‰å…¥CLIPæ¨¡å‹ï¼Œå› ç‚ºç•¶å‰é‹ç®—è¨­å‚™æ˜¯ {self.device}ã€‚\")\n",
        "        return self.clip_model, self.clip_preprocess\n",
        "\n",
        "    def load_sd_model(self, model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
        "        print(f\"6:æº–å‚™è¼‰å…¥ Stable Diffusion æ¨¡å‹ ({model_id})...\")\n",
        "        if self.device == \"cuda\":\n",
        "            try:\n",
        "                self.sd_pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "                self.sd_pipeline = self.sd_pipeline.to(self.device)\n",
        "                print(f\"  Stable Diffusion æ¨¡å‹ ({model_id}) å·²æˆåŠŸè¼‰å…¥åˆ° {self.device}ï¼\")\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                print(f\"  è¼‰å…¥Stable Diffusionæ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "                self.sd_pipeline = None\n",
        "        else:\n",
        "            print(f\"  æœªå¯¦éš›è¼‰å…¥Stable Diffusionæ¨¡å‹ï¼Œå› ç‚ºç•¶å‰é‹ç®—è¨­å‚™æ˜¯ {self.device}ã€‚\")\n",
        "        return self.sd_pipeline\n",
        "\n",
        "    def init_translation_models(self, target_languages=['en', 'ja', 'ko']):\n",
        "        print(\"7:æ­£åœ¨åˆå§‹åŒ– Hugging Face ç¿»è­¯æ¨¡å‹...\")\n",
        "        model_map = {\n",
        "            'en': 'Helsinki-NLP/opus-mt-zh-en',\n",
        "            'ja': 'Helsinki-NLP/opus-mt-tc-big-zh-ja', # ç¶²è·¯ä¸Šè‡ªå·±æ‰¾åˆ°çš„æ¨¡å‹\n",
        "            'ko': 'Helsinki-NLP/opus-mt-tc-big-zh-ja'  # ç¶²è·¯ä¸Šè‡ªå·±æ‰¾åˆ°çš„æ¨¡å‹\n",
        "        }\n",
        "        loaded_any_model = False\n",
        "        for lang_code in target_languages:\n",
        "            if lang_code in model_map:\n",
        "                model_name = model_map[lang_code]\n",
        "                try:\n",
        "                    print(f\"  è¼‰å…¥ç¿»è­¯æ¨¡å‹ for zh -> {lang_code} ({model_name})...\")\n",
        "                    translator = pipeline(f\"translation_zh_to_{lang_code}\", # ä»»å‹™åå¯èƒ½éœ€è¦èª¿æ•´\n",
        "                                          model=model_name,\n",
        "                                          device=self.hf_device_id)\n",
        "                    self.translation_pipelines[lang_code] = translator\n",
        "                    print(f\"    ç¿»è­¯æ¨¡å‹ for zh -> {lang_code} ({model_name}) è¼‰å…¥æˆåŠŸã€‚\")\n",
        "                    loaded_any_model = True\n",
        "                except Exception as e:\n",
        "                    print(f\"    è¼‰å…¥ç¿»è­¯æ¨¡å‹ for zh -> {lang_code} ({model_name}) å¤±æ•—: {e}\")\n",
        "                    self.translation_pipelines[lang_code] = None\n",
        "            else:\n",
        "                print(f\"  æœªæ‰¾åˆ°é‡å° zh -> {lang_code} çš„é å®šç¾©ç¿»è­¯æ¨¡å‹ã€‚\")\n",
        "        if not loaded_any_model:\n",
        "            print(\"è­¦å‘Š: æœªèƒ½æˆåŠŸè¼‰å…¥ä»»ä½•Hugging Faceç¿»è­¯æ¨¡å‹ã€‚è‡ªå‹•ç¿»è­¯åŠŸèƒ½å°‡å—é™ã€‚\")\n",
        "        return self.translation_pipelines\n",
        "\n",
        "    def cleanup(self):\n",
        "        print(\"æ­£åœ¨æ¸…ç†æ¨¡å‹è³‡æº...\")\n",
        "        if self.clip_model: del self.clip_model\n",
        "        if self.clip_preprocess: del self.clip_preprocess\n",
        "        if self.sd_pipeline: del self.sd_pipeline\n",
        "        if self.translation_pipelines:\n",
        "            for lang, pipe in self.translation_pipelines.items():\n",
        "                if pipe: del pipe\n",
        "            self.translation_pipelines.clear()\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"  æ¨¡å‹å’Œè³‡æºæ¸…ç†æ“ä½œå·²åŸ·è¡Œã€‚\")"
      ],
      "metadata": {
        "id": "WGwczTjNvDFN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ç”¨ä¾†è™•ç†:è©å½™æ¦‚å¿µçš„å»ºç«‹èˆ‡ç®¡ç†ï¼Œ\"å¸Œæœ›èƒ½\"å¹«æˆ‘æŠŠä¸€å€‹ã€Œä¸­æ–‡æ¦‚å¿µã€ç¿»è­¯æˆå¤šç¨®èªè¨€ç‰ˆæœ¬ï¼ˆä¸­ã€è‹±ã€æ—¥ã€éŸ“ï¼‰\n",
        "#ä¸¦ä¸”è¨˜éŒ„æ¯å€‹å–®å­—ã€åŸå§‹ä¸­æ–‡ã€å’Œå„èªè¨€çš„è©å½™èªªæ˜(é€™éƒ¨åˆ†ä¸€ç›´åšä¸å¤ªæˆåŠŸ)\n",
        "#ç¸½ä¹‹å°±æ˜¯è‡ªå‹•ç¿»è­¯ï¼Œä¹Ÿè«‹aiå¹«æˆ‘åšéå¥½å¹¾ç‰ˆï¼Œçµæœéƒ½ä¸å¤ªå¥½ï¼Œåˆç¶“éå¥½å¹¾æ¬¡è‡ªå·±æ‰‹å‹•æ”¹è·Ÿæ‘¸ç´¢......\n",
        "\n",
        "class ConceptDataProvider:\n",
        "    def __init__(self, clear_default_concepts=False): # æ–°å¢åƒæ•¸\n",
        "        self.word_concepts_list = [] # é»˜èªæ¸…ç©ºï¼Œç”±ç”¨æˆ¶åœ¨mainä¸­æ·»åŠ \n",
        "        if not clear_default_concepts:\n",
        "            # å¦‚æœéœ€è¦ï¼Œå¯ä»¥ä¿ç•™åŸæœ‰çš„é è¨­åˆ—è¡¨\n",
        "            self.word_concepts_list = [\n",
        "                {\"concept_name\": \"cool_ambiguous\", \"base_chinese\": \"é…· / æ¶¼çˆ½\", \"translations\": {\"zh\": \"å†°æ¶¼çš„é£²æ–™ï¼Œé…·ç‚«çš„é¢¨æ ¼ï¼Œå†·éœçš„æ…‹åº¦\", \"en\": \"cool refreshing drink, cool stylish look, calm and cool attitude\", \"ja\": \"å†·ãŸã„é£²ã¿ç‰©ã€ã‹ã£ã“ã„ã„ã‚¹ã‚¿ã‚¤ãƒ«ã€å†·é™ãªæ…‹åº¦\", \"ko\": \"ì‹œì›í•œ ìŒë£Œ, ë©‹ì§„ ìŠ¤íƒ€ì¼, ì¹¨ì°©í•œ íƒœë„\"}},\n",
        "\n",
        "            ]\n",
        "        self.word_concepts_to_process = self.word_concepts_list\n",
        "        print(f\"8:åˆå§‹å®šç¾©äº† {len(self.word_concepts_list)} å€‹è©å½™æ¦‚å¿µã€‚\")\n",
        "\n",
        "    def _generate_concept_name(self, base_chinese):\n",
        "        \"\"\"æ ¹æ“šä¸­æ–‡è©å½™ç”Ÿæˆä¸€å€‹ç°¡åŒ–çš„è‹±æ–‡æ¦‚å¿µå\"\"\"\n",
        "        name = re.sub(r'[^\\w]', '', base_chinese) #ç§»é™¤éå­—æ¯æ•¸å­—å­—ç¬¦(æ¸¬è©¦ä¸­ç¸½æ˜¯äº‚ç¢¼)\n",
        "        name = name[:20] #é™åˆ¶é•·åº¦\n",
        "        if not name: name = \"unnamed_concept\"\n",
        "        return f\"{name}_auto_translated\"\n",
        "\n",
        "    #è‡ªå‹•å‘¼å«ç¿»è­¯æ¨¡å‹ï¼Œæ ¸å¿ƒåŠŸèƒ½~~~\n",
        "    def add_concept_with_auto_translation(self, translation_pipelines, base_chinese,\n",
        "                                          chinese_description_for_prompt=None, target_languages=['en', 'ja', 'ko']):\n",
        "        \"\"\"\n",
        "        æ–°å¢ä¸€å€‹æ¦‚å¿µï¼Œä¸¦ä½¿ç”¨ Hugging Face pipeline è‡ªå‹•ç¿»è­¯å…¶æè¿°æ€§å¥å­ã€‚\n",
        "        å¦‚æœ chinese_description_for_prompt æœªæä¾›ï¼Œå‰‡ç›´æ¥ç¿»è­¯ base_chineseã€‚\n",
        "        \"\"\"\n",
        "        concept_name = self._generate_concept_name(base_chinese)\n",
        "        text_to_translate = chinese_description_for_prompt if chinese_description_for_prompt else base_chinese\n",
        "\n",
        "        print(f\"\\n  æº–å‚™æ–°å¢æ¦‚å¿µ '{concept_name}' (åŸºæ–¼ '{base_chinese}')...\")\n",
        "        print(f\"    å°‡ç¿»è­¯: '{text_to_translate[:50]}...'\")\n",
        "\n",
        "        if not translation_pipelines or not any(translation_pipelines.values()):\n",
        "            print(\"    éŒ¯èª¤ï¼šç¿»è­¯ pipelines æœªæä¾›æˆ–å‡æœªæˆåŠŸè¼‰å…¥ï¼Œç„¡æ³•è‡ªå‹•ç¿»è­¯ã€‚å°‡ä½¿ç”¨åŸæ–‡ä½œç‚ºæç¤ºã€‚\")\n",
        "            translations = {lang: text_to_translate for lang in target_languages}\n",
        "            translations['zh'] = text_to_translate\n",
        "        else:\n",
        "            translations = {\"zh\": text_to_translate}\n",
        "            for lang_code in target_languages:\n",
        "                translator_pipeline = translation_pipelines.get(lang_code)\n",
        "                if translator_pipeline:\n",
        "                    try:\n",
        "                        translated_result = translator_pipeline(text_to_translate)\n",
        "                        translated_text = translated_result[0]['translation_text']\n",
        "                        translations[lang_code] = translated_text\n",
        "                        print(f\"      -> {lang_code.upper()}: {translated_text}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"      ä½¿ç”¨Hugging Faceæ¨¡å‹ç¿»è­¯åˆ° {lang_code.upper()} å¤±æ•—: {e}\")\n",
        "\n",
        "                        #åŸæœ¬çš„AIç”Ÿæˆ\n",
        "                        # translations[lang_code] = f\"ç¿»è­¯å¤±æ•—: {text_to_translate}\"\n",
        "                        #æ›´æ”¹\n",
        "                        translations[lang_code] = text_to_translate\n",
        "                #åŸæœ¬çš„AIç”Ÿæˆ\n",
        "                # else:\n",
        "                #     print(f\"      æœªæ‰¾åˆ° {lang_code.upper()} çš„ç¿»è­¯ pipelineï¼Œä½¿ç”¨åŸæ–‡ã€‚\")\n",
        "                #     translations[lang_code] = text_to_translate\n",
        "                #æ›´æ”¹\n",
        "                else:\n",
        "                      print(f\"æœªæ‰¾åˆ°{lang_code.upper()}çš„ç¿»è­¯pipelineï¼Œè©²èªè¨€å°‡è·³éã€‚\")\n",
        "                #ä¸åŠ å…¥translationsï¼Œå¾Œé¢è‡ªå‹•å°ä¸å­˜åœ¨èªè¨€ç•¥é\n",
        "\n",
        "        new_concept = {\n",
        "            \"concept_name\": concept_name,\n",
        "            \"base_chinese\": base_chinese,\n",
        "            \"translations\": translations\n",
        "        }\n",
        "        self.word_concepts_list.append(new_concept)\n",
        "        print(f\"  æ–°æ¦‚å¿µ '{concept_name}' å·²æˆåŠŸæ·»åŠ ã€‚ç¾æœ‰ {len(self.word_concepts_list)} å€‹æ¦‚å¿µã€‚\")\n",
        "        print(f\"8(æ›´æ–°):å®šç¾©äº† {len(self.word_concepts_to_process)} å€‹è©å½™æ¦‚å¿µç”¨æ–¼æœ¬æ¬¡åˆ†æã€‚\")\n",
        "\n",
        "    def get_concepts_to_process(self):\n",
        "        if not self.word_concepts_to_process:\n",
        "            print(\"è­¦å‘Šï¼šæ²’æœ‰å®šç¾©ä»»ä½•è©å½™æ¦‚å¿µé€²è¡Œè™•ç†ã€‚\")\n",
        "        return self.word_concepts_to_process"
      ],
      "metadata": {
        "id": "w4a_ashAvGdl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ä¸»è¦è™•ç†CLIPçš„æ–‡æœ¬åˆ†æåŠŸèƒ½\n",
        "#ä½œç”¨1.æŠŠå¤šèªè¨€çš„æç¤ºè©é€é€²CLIPæ¨¡å‹ï¼Œè½‰æ›æˆå‘é‡\n",
        "#ä½œç”¨2.æ¯”è¼ƒä¸åŒèªè¨€ä¹‹é–“çš„èªæ„ç›¸ä¼¼åº¦\n",
        "#å¸Œæœ›èƒ½ä»¥æ­¤åˆ¤æ–·ã€Œç¿»è­¯å¾Œçš„æç¤ºè©ã€åœ¨èªæ„ä¸Šæ˜¯ä¸æ˜¯ç›¸è¿‘\n",
        "#é€™æ®µåŸæœ¬ä¹Ÿæ˜¯aiç”Ÿæˆçš„ï¼Œç¶“éæˆ‘çš„ç†è§£èˆ‡æ•´ç†:\n",
        "\n",
        "class TextAnalyzer:\n",
        "    def __init__(self, clip_model, device):\n",
        "        self.clip_model = clip_model\n",
        "        self.device = device\n",
        "    def get_clip_text_embeddings(self, text_prompts_dict):\n",
        "        if self.clip_model is None:\n",
        "            print(\"  è­¦å‘Š: CLIP æ¨¡å‹æœªè¼‰å…¥ï¼Œæ–‡æœ¬åµŒå…¥å°‡ç‚ºé›¶å‘é‡ã€‚\")\n",
        "            return {lang_code: np.zeros(512, dtype=np.float32) for lang_code in text_prompts_dict}\n",
        "        text_embeddings_result_dict = {}\n",
        "        with torch.no_grad():\n",
        "            for lang_tag, text_content in text_prompts_dict.items():\n",
        "                try:\n",
        "                    tokenized_input_text = clip.tokenize([text_content]).to(self.device)\n",
        "                    text_semantic_features = self.clip_model.encode_text(tokenized_input_text)\n",
        "                    text_semantic_features /= text_semantic_features.norm(dim=-1, keepdim=True)\n",
        "                    text_embeddings_result_dict[lang_tag] = text_semantic_features.cpu().numpy().flatten()\n",
        "                except Exception as e:\n",
        "                    print(f\"ç‚º '{lang_tag}':'{text_content[:30]}...' ç”ŸæˆCLIPåµŒå…¥æ™‚å‡ºéŒ¯: {e}\")\n",
        "                    text_embeddings_result_dict[lang_tag] = np.zeros(512, dtype=np.float32)\n",
        "        return text_embeddings_result_dict\n",
        "    def calculate_embedding_similarity(self, embeddings_dict, reference_lang='en'):\n",
        "        similarity_scores_result = {}\n",
        "        if reference_lang not in embeddings_dict or embeddings_dict.get(reference_lang) is None or np.all(np.isclose(embeddings_dict[reference_lang], 0)):\n",
        "            print(f\"  åƒè€ƒèªè¨€ '{reference_lang.upper()}' çš„åµŒå…¥å‘é‡ç„¡æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œç„¡æ³•è¨ˆç®—ç›¸ä¼¼åº¦ã€‚\")\n",
        "            return {f\"{reference_lang}_vs_{lang}\": None for lang in embeddings_dict if lang != reference_lang}\n",
        "        print(f\"  CLIPæ–‡æœ¬åµŒå…¥å‘é‡é¤˜å¼¦ç›¸ä¼¼åº¦ (vs '{reference_lang.upper()}'):\")\n",
        "        ref_embedding = embeddings_dict[reference_lang].reshape(1, -1)\n",
        "        for lang, emb in embeddings_dict.items():\n",
        "            if lang == reference_lang: continue\n",
        "            sim_val_str = \"N/A (åµŒå…¥ç„¡æ•ˆ)\"\n",
        "            sim_num = None\n",
        "            if emb is not None and not np.all(np.isclose(emb, 0)):\n",
        "                sim_num = cosine_similarity(ref_embedding, emb.reshape(1, -1))[0][0]\n",
        "                sim_val_str = f\"{sim_num:.3f}\"\n",
        "            similarity_scores_result[f\"{reference_lang}_vs_{lang}\"] = sim_num\n",
        "            print(f\"    - èˆ‡ {lang.upper()}: {sim_val_str}\")\n",
        "        return similarity_scores_result"
      ],
      "metadata": {
        "id": "Dh5nle1GvHpl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#è™•ç†ã€Œåœ–åƒç”Ÿæˆèˆ‡åˆ†æã€çš„åŠŸèƒ½\n",
        "#ä½œç”¨1.ç”¨Stable Diffusionæ¨¡å‹æ ¹æ“šæç¤ºè©ç”Ÿæˆåœ–ç‰‡(ç›®å‰æˆæ•ˆä¸ä½³)\n",
        "#ä½œç”¨2.åœ–ç‰‡ä¸­æå–å‡ºä¸»è‰²\n",
        "#ä½œç”¨3.åˆ†æåœ–ç‰‡çš„æ•´é«”äº®åº¦ã€å°æ¯”ã€é£½å’Œåº¦\n",
        "\n",
        "class ImageProcessor:\n",
        "    def __init__(self, sd_pipeline, device):\n",
        "        self.sd_pipeline = sd_pipeline\n",
        "        self.device = device\n",
        "\n",
        "    #æ¸¬è©¦\n",
        "    def generate_image_from_prompt(self, prompt):\n",
        "        image = self.sd_pipeline(prompt).images[0]\n",
        "        return image\n",
        "\n",
        "\n",
        "    def generate_image_with_sd(self, prompt_text, random_seed=42, inference_steps=30, cfg_scale=7.5):\n",
        "        if self.sd_pipeline is None:\n",
        "            placeholder_img = Image.new('RGB', (512, 512), color='silver')\n",
        "            draw = ImageDraw.Draw(placeholder_img)\n",
        "            try: font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n",
        "            except IOError: font = ImageFont.load_default()\n",
        "            draw.text((10, 10), f\"SDæ¨¡å‹æœªè¼‰å…¥\\næç¤º:\\n{prompt_text[:70]}...\", fill=(60, 60, 60), font=font)\n",
        "            return placeholder_img\n",
        "        try:\n",
        "            gen = torch.Generator(device=self.device).manual_seed(random_seed)\n",
        "            with torch.no_grad():\n",
        "                img = self.sd_pipeline(prompt_text, num_inference_steps=inference_steps, guidance_scale=cfg_scale, generator=gen).images[0]\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"  ç”Ÿæˆåœ–åƒæ™‚å‡ºéŒ¯ ('{prompt_text[:40]}...'): {e}\")\n",
        "            error_img = Image.new('RGB', (512, 512), color='lightcoral')\n",
        "            draw = ImageDraw.Draw(error_img)\n",
        "            try: font = ImageFont.truetype(\"DejaVuSans.ttf\", 15)\n",
        "            except IOError: font = ImageFont.load_default()\n",
        "            draw.text((10, 10), f\"åœ–åƒç”ŸæˆéŒ¯èª¤:\\n{prompt_text[:60]}...\\néŒ¯èª¤:\\n{str(e)[:100]}\", fill=(0, 0, 0), font=font)\n",
        "            return error_img\n",
        "    def extract_dominant_colors(self, pil_img, num_colors=5):\n",
        "\n",
        "        #åŸæœ¬çš„AIç”Ÿæˆ\n",
        "        # if pil_img is None or (hasattr(pil_img, 'width') and pil_img.width < num_colors) or \\\n",
        "        #    (hasattr(pil_img, 'height') and pil_img.height < num_colors):\n",
        "\n",
        "        #æ›´æ”¹\n",
        "        if pil_img is None or pil_img.width * pil_img.height < num_colors * 4:\n",
        "             rgb_fallback = np.array([[128, 128, 128]] * num_colors, dtype=int)\n",
        "\n",
        "             rgb_fallback = np.array([[128, 128, 128]] * num_colors, dtype=int)\n",
        "             lab_fallback = rgb2lab(rgb_fallback.reshape((num_colors, 1, 3)) / 255.0).reshape((num_colors, 3))\n",
        "             return rgb_fallback, lab_fallback\n",
        "        try:\n",
        "            img_rgb = pil_img.convert('RGB')\n",
        "            max_dim = 150\n",
        "            ratio = max_dim / max(img_rgb.width, img_rgb.height)\n",
        "            new_size = (max(1, int(img_rgb.width * ratio)), max(1, int(img_rgb.height * ratio)))\n",
        "            img_res = img_rgb.resize(new_size, Image.Resampling.LANCZOS)\n",
        "            pixels = np.array(img_res).reshape(-1, 3)\n",
        "            if pixels.shape[0] < num_colors:\n",
        "                rgb_colors = np.zeros((num_colors, 3), dtype=int)\n",
        "                actual_extracted_colors = pixels.astype(int)\n",
        "                rgb_colors[:actual_extracted_colors.shape[0]] = actual_extracted_colors\n",
        "                if actual_extracted_colors.shape[0] < num_colors:\n",
        "                    rgb_colors[actual_extracted_colors.shape[0]:] = np.array([128,128,128])\n",
        "            else:\n",
        "                kmeans = KMeans(n_clusters=num_colors, random_state=0, n_init='auto', max_iter=200).fit(pixels)\n",
        "                rgb_colors = kmeans.cluster_centers_.astype(int)\n",
        "            if rgb_colors.shape[0] < num_colors:\n",
        "                padded_colors = np.full((num_colors, 3), 128, dtype=int)\n",
        "                padded_colors[:rgb_colors.shape[0]] = rgb_colors\n",
        "                rgb_colors = padded_colors\n",
        "            lab_colors = rgb2lab(rgb_colors.reshape((num_colors, 1, 3)) / 255.0).reshape((num_colors, 3))\n",
        "            return rgb_colors, lab_colors\n",
        "        except Exception as e:\n",
        "            print(f\"  æå–ä¸»è‰²èª¿æ™‚å‡ºéŒ¯: {e}\")\n",
        "            rgb_err = np.array([[100, 100, 100]] * num_colors, dtype=int)\n",
        "            lab_err = rgb2lab(rgb_err.reshape((num_colors,1,3)) / 255.0).reshape((num_colors, 3))\n",
        "            return rgb_err, lab_err\n",
        "    def analyze_global_features(self, pil_image):\n",
        "        if pil_image is None: return {\"avg_brightness\": \"N/A\", \"contrast_std\": \"N/A\", \"avg_saturation\": \"N/A\"}\n",
        "        try:\n",
        "            cv_bgr = np.array(pil_image.convert('RGB'))[:, :, ::-1].copy()\n",
        "            gray = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2GRAY)\n",
        "            brightness = round(np.mean(gray), 2)\n",
        "            contrast = round(np.std(gray), 2)\n",
        "            hsv = cv2.cvtColor(cv_bgr, cv2.COLOR_BGR2HSV)\n",
        "            saturation = round(np.mean(hsv[:, :, 1]), 2)\n",
        "            return {\"avg_brightness\": brightness, \"contrast_std\": contrast, \"avg_saturation\": saturation}\n",
        "        except Exception as e:\n",
        "            print(f\"  åˆ†æå…¨å±€åœ–åƒç‰¹å¾µæ™‚å‡ºéŒ¯: {e}\")\n",
        "            return {\"avg_brightness\": \"Err\", \"contrast_std\": \"Err\", \"avg_saturation\": \"Err\"}"
      ],
      "metadata": {
        "id": "v1Dmo3XwvI8d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#åˆ†æå ±å‘Š\n",
        "#ä½œç”¨1.æ ¹æ“šæ¦‚å¿µå’Œåˆ†æçµæœï¼Œç”¢å‡ºæ–‡å­—èªªæ˜æ¡†æ¶\n",
        "#ä½œç”¨2.æŠŠåœ–ç‰‡ã€é¡è‰²ã€CLIPç›¸ä¼¼åº¦ã€ç‰¹å¾µç­‰ç­‰ç•«æˆä¸€å¼µå®Œæ•´æ¯”è¼ƒåœ–\n",
        "#ä»¥ä¸Šå°±æ˜¯æ–‡å­—ç†è§£~åœ–åƒç”Ÿæˆ~çµæœåˆ†æ~å ±å‘Šè¼¸å‡º\n",
        "\n",
        "class ReportGenerator:\n",
        "    def generate_explanation_template(self, chinese_concept, lang_prompt_info, dom_colors_hex=None, global_feats=None):\n",
        "        expl = f\"\\n--- è§£é‡‹æ¨¡æ¿ for æ¦‚å¿µ:ã€{chinese_concept}ã€‘| èªè¨€æç¤º: ã€{lang_prompt_info[:70]}...ã€‘ ---\\n\"\n",
        "        if dom_colors_hex: expl += f\"åœ–åƒä¸»è¦è‰²ç¥¨ (HEX): {', '.join(dom_colors_hex[:3])} ...\\n\"\n",
        "        if global_feats:\n",
        "            expl += f\"å…¨å±€åœ–åƒç‰¹å¾µ: äº®åº¦={global_feats.get('avg_brightness', 'N/A')}, \"\n",
        "            expl += f\"å°æ¯”åº¦={global_feats.get('contrast_std', 'N/A')}, é£½å’Œåº¦={global_feats.get('avg_saturation', 'N/A')}\\n\"\n",
        "        expl += f\"\\nåŸå› æ¨æ¸¬èˆ‡åœ–åƒæè¿° (è«‹æ‚¨å¡«å……)ï¼š\\n\"\n",
        "        expl += f\"   [è«‹çµåˆä»¥ä¸Šå®¢è§€æŒ‡æ¨™å’Œæ‚¨çš„è§€å¯Ÿï¼Œè©³ç´°é—¡è¿°ï¼š\\n\"\n",
        "        expl += f\"    a. åœ–åƒè¦–è¦ºé¢¨æ ¼èˆ‡æ°›åœï¼Ÿ\\n    b. ä¸»è¦å…ƒç´ èˆ‡æç¤ºè©çš„é—œè¯ï¼Ÿ\\n    c. è‰²å½©é‹ç”¨å¦‚ä½•è©®é‡‹æç¤ºè©ï¼Ÿ\\n\"\n",
        "        expl += f\"    d. (ç‰¹å®šèªè¨€)æ–‡åŒ–èƒŒæ™¯çš„å¯èƒ½å½±éŸ¿ï¼Ÿ\\n    e. èˆ‡å…¶ä»–èªè¨€ç”Ÿæˆåœ–åƒçš„å·®ç•°åŠå¯èƒ½åŸå› ï¼Ÿ]\\n\"\n",
        "        expl += f\"--------------------------------------------------------------------------\\n\"\n",
        "        return expl\n",
        "    def plot_concept_results(self, concept_id, base_chinese, prompts_dict, images_dict,colors_dict, similarities_dict, global_features_dict=None):\n",
        "        langs = list(prompts_dict.keys())\n",
        "        num_langs = len(langs)\n",
        "        if num_langs == 0:\n",
        "            print(f\"æ¦‚å¿µ '{concept_id}' ç„¡ prompts å¯ç¹ªè£½ã€‚\")\n",
        "            return\n",
        "        h_ratio, w_ratio = 2.8, 4.0\n",
        "        total_h, total_w = h_ratio * 2, w_ratio * num_langs if num_langs > 0 else w_ratio\n",
        "        fig, axs = plt.subplots(2, max(1, num_langs), figsize=(total_w, total_h), gridspec_kw={'height_ratios': [0.78, 0.22]})\n",
        "        if num_langs == 1: axs = axs.reshape(2, 1)\n",
        "\n",
        "        title_base = f\"æ¦‚å¿µåˆ†æ: '{base_chinese}' ({concept_id})\\nCLIPç›¸ä¼¼åº¦(vs EN): \"\n",
        "        sim_strs = []\n",
        "        if similarities_dict:\n",
        "            sim_strs = [f\"{k.split('_vs_')[-1].upper()}: {v:.2f}\" if isinstance(v, (float, np.floating)) else f\"{k.split('_vs_')[-1].upper()}: {v}\"\n",
        "                        for k, v in similarities_dict.items()]\n",
        "        fig.suptitle(title_base + \", \".join(sim_strs), fontsize=11, y=1.04)\n",
        "        for i, lang in enumerate(langs):\n",
        "            img = images_dict.get(lang)\n",
        "            colors_data = colors_dict.get(lang)\n",
        "            global_feats_this_lang = (global_features_dict or {}).get(lang, {})\n",
        "            ax_img = axs[0, i]; ax_color = axs[1, i]\n",
        "            if img: ax_img.imshow(img)\n",
        "            else: ax_img.text(0.5, 0.5, 'åœ–åƒæœªç”Ÿæˆ', ha='center', va='center', transform=ax_img.transAxes)\n",
        "            img_title_prompt = prompts_dict.get(lang, \"N/A\")\n",
        "            img_title = f\"{lang.upper()}: \\\"{img_title_prompt[:30]}\\\"...\"\n",
        "            if global_feats_this_lang:\n",
        "                img_title += f\"\\näº®:{global_feats_this_lang.get('avg_brightness', '-')} å°æ¯”:{global_feats_this_lang.get('contrast_std', '-')} é£½:{global_feats_this_lang.get('avg_saturation', '-')}\"\n",
        "            ax_img.set_title(img_title, fontsize=7.5); ax_img.axis('off')\n",
        "            if colors_data:\n",
        "                rgb_patch, lab_patch = colors_data\n",
        "                if rgb_patch is not None and len(rgb_patch) > 0 :\n",
        "                    n_patch = len(rgb_patch)\n",
        "                    patch_canvas = np.zeros((25, 100, 3), dtype=np.uint8)\n",
        "                    patch_w = 100 // n_patch\n",
        "                    for j, rgb_c in enumerate(rgb_patch): patch_canvas[:, j * patch_w:(j + 1) * patch_w] = rgb_c\n",
        "                    ax_color.imshow(patch_canvas)\n",
        "                    lab_str_parts = []\n",
        "                    if lab_patch is not None:\n",
        "                        for l_val, a_val, b_val in lab_patch[:min(3,n_patch)]: lab_str_parts.append(f\"L{l_val:.0f} a{a_val:.0f} b{b_val:.0f}\")\n",
        "                        lab_str = \"\\n\".join(lab_str_parts)\n",
        "                        ax_color.set_title(f\"Lab(Top{min(3,n_patch)}):\\n{lab_str}\", fontsize=6)\n",
        "                    else: ax_color.set_title(f\"RGBé¡è‰²\", fontsize=6)\n",
        "                else: ax_color.text(0.5,0.5,'ç„¡é¡è‰²æ•¸æ“š',ha='center',va='center',transform=ax_color.transAxes, fontsize=6)\n",
        "            else: ax_color.text(0.5, 0.5, 'ç„¡é¡è‰²', ha='center', va='center', transform=ax_color.transAxes, fontsize=6)\n",
        "            ax_color.axis('off')\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.94]); plt.subplots_adjust(hspace=0.5, wspace=0.3); plt.show()"
      ],
      "metadata": {
        "id": "NGGBRHnJvKVl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ä¸²æ¥æ‰€æœ‰æ¨¡çµ„(æ–‡å­—åˆ†æ+åœ–åƒè™•ç†+å ±å‘Šç”Ÿæˆ)\n",
        "#é™¤éŒ¯èª¤åŠ äº†if prompt is None:#é‡åˆ°ç¿»è­¯ç¼ºå¤±å°±è·³\n",
        "\n",
        "class AnalysisPipeline:\n",
        "    def __init__(self, config_manager, model_manager, concept_provider, text_analyzer, image_processor, report_generator):\n",
        "        self.config = config_manager\n",
        "        self.models = model_manager\n",
        "        self.concepts_provider = concept_provider\n",
        "        self.text_analyzer = text_analyzer\n",
        "        self.image_processor = image_processor\n",
        "        self.reporter = report_generator\n",
        "        self.base_seed = 20240101\n",
        "        self.sd_steps = 20\n",
        "        self.sd_cfg = 7.0  # æ¸›å°‘æ­¥æ•¸ä»¥åŠ é€Ÿ\n",
        "        self.num_dom_colors = 5\n",
        "        self.save_images_flag = True\n",
        "\n",
        "    def run_analysis(self):\n",
        "        print(f\"Cell 15: å³å°‡é–‹å§‹åŸ·è¡Œä¸»æµç¨‹...\")\n",
        "        concepts_to_process = self.concepts_provider.get_concepts_to_process()\n",
        "        if not concepts_to_process:\n",
        "            print(\"æ²’æœ‰æ¦‚å¿µéœ€è¦è™•ç†ï¼Œæµç¨‹çµæŸã€‚\")\n",
        "            return []\n",
        "        print(f\"  å°‡è™•ç† {len(concepts_to_process)} å€‹è©å½™æ¦‚å¿µ...\")\n",
        "        results_collection = []\n",
        "        for concept_idx, concept_detail in enumerate(tqdm(concepts_to_process, desc=\"ç¸½é«”æ¦‚å¿µè™•ç†\")):\n",
        "            concept_id = concept_detail[\"concept_name\"]\n",
        "            base_zh = concept_detail[\"base_chinese\"]\n",
        "            prompts = concept_detail[\"translations\"]\n",
        "            print(f\"\\n\\nè™•ç†æ¦‚å¿µ #{concept_idx + 1}: '{base_zh}' ({concept_id})\")\n",
        "            print(\"  [1. CLIPåµŒå…¥åˆ†æ]\")\n",
        "            embeddings = self.text_analyzer.get_clip_text_embeddings(prompts)\n",
        "            similarities = self.text_analyzer.calculate_embedding_similarity(embeddings, reference_lang='en')\n",
        "            print(\"  [2. åœ–åƒç”Ÿæˆã€é¡è‰²èˆ‡å…¨å±€ç‰¹å¾µåˆ†æ]\")\n",
        "            concept_images = {}\n",
        "            concept_colors = {}\n",
        "            concept_global_features = {}\n",
        "            concept_explanations_str = \"\"\n",
        "\n",
        "            for lang_idx, (lang, prompt) in enumerate(tqdm(prompts.items(), desc=f\"  '{concept_id}'èªè¨€è™•ç†\", leave=False)):\n",
        "                if not prompt or not prompt.strip():  # é‡åˆ°ç¿»è­¯ç¼ºå¤±æˆ–ç©ºç™½å°±è·³é\n",
        "                    print(f\"    -> {lang.upper()}: âš ï¸ ç¿»è­¯ç¼ºå¤±æˆ–ç©ºç™½ï¼Œè·³éã€‚\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"    -> {lang.upper()}: '{prompt}'\")\n",
        "                img_seed = self.base_seed + concept_idx * 100 + lang_idx * 10\n",
        "\n",
        "                pil_img = self.image_processor.generate_image_with_sd(\n",
        "                    prompt,\n",
        "                    random_seed=img_seed,\n",
        "                    inference_steps=self.sd_steps,\n",
        "                    cfg_scale=self.sd_cfg\n",
        "                )\n",
        "                concept_images[lang] = pil_img\n",
        "\n",
        "                if self.save_images_flag and pil_img:\n",
        "                    try:\n",
        "                        fname = f\"{concept_id}_{lang}_s{img_seed}.png\"\n",
        "                        fpath = os.path.join(self.config.images_out_dir, fname)\n",
        "                        pil_img.save(fpath)\n",
        "                    except Exception as e:\n",
        "                        print(f\"      å„²å­˜åœ–åƒ'{fname}'å¤±æ•—: {e}\")\n",
        "\n",
        "                rgb_cs, lab_cs = self.image_processor.extract_dominant_colors(pil_img, self.num_dom_colors)\n",
        "                concept_colors[lang] = (rgb_cs, lab_cs)\n",
        "\n",
        "                global_feats = self.image_processor.analyze_global_features(pil_img)\n",
        "                concept_global_features[lang] = global_feats\n",
        "\n",
        "                hex_colors = [f\"#{c[0]:02x}{c[1]:02x}{c[2]:02x}\" for c in rgb_cs] if rgb_cs and len(rgb_cs) > 0 else []\n",
        "\n",
        "                expl_text = self.reporter.generate_explanation_template(\n",
        "                    base_zh,\n",
        "                    f\"{lang.upper()}: {prompt}\",\n",
        "                    hex_colors,\n",
        "                    global_feats\n",
        "                )\n",
        "                print(expl_text)\n",
        "                concept_explanations_str += expl_text\n",
        "\n",
        "                if self.config.device == \"cuda\":\n",
        "                    torch.cuda.empty_cache()\n",
        "                    time.sleep(0.05)\n",
        "\n",
        "            print(\"\\n  [3. ç¹ªè£½çµæœåœ–è¡¨]\")\n",
        "            self.reporter.plot_concept_results(\n",
        "                concept_id,\n",
        "                base_zh,\n",
        "                prompts,\n",
        "                concept_images,\n",
        "                concept_colors,\n",
        "                similarities,\n",
        "                concept_global_features\n",
        "            )\n",
        "            results_collection.append({\n",
        "                \"concept\": concept_id,\n",
        "                \"base_chinese\": base_zh,\n",
        "                \"prompts\": prompts,\n",
        "                \"similarities\": similarities,\n",
        "                \"global_features\": concept_global_features,\n",
        "                \"explanation_prompts_combined\": concept_explanations_str\n",
        "            })\n",
        "            print(f\"  æ¦‚å¿µ '{concept_id}' åˆ†æå®Œç•¢ã€‚\")\n",
        "\n",
        "            if self.config.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        print(\"\\n\\næ‰€æœ‰è©å½™æ¦‚å¿µè™•ç†å®Œæˆï¼è§£é‡‹æ¨¡æ¿å·²åœ¨ä¸Šæ–¹æ‰“å°ã€‚\")\n",
        "        return results_collection\n"
      ],
      "metadata": {
        "id": "jOp8Pt8zvLg9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.åŸ·è¡Œåˆ†ææµç¨‹æˆ–è·³é\n",
        "concept_provider = ConceptDataProvider(clear_default_concepts=True)\n",
        "text_analyzer = TextAnalyzer(clip_model=clip_model, device=config_mgr.device)\n",
        "image_processor = ImageProcessor(sd_pipeline=sd_pipeline, device=config_mgr.device)\n",
        "report_generator = ReportGenerator()\n",
        "\n",
        "if not concept_provider.get_concepts_to_process():\n",
        "    print(\"æ²’æœ‰ä»»ä½•æ¦‚å¿µè¢«å®šç¾©ï¼Œç„¡æ³•åŸ·è¡Œåˆ†ææµç¨‹ã€‚è«‹æª¢æŸ¥ `chinese_terms_to_analyze`ã€‚\")\n",
        "else:\n",
        "    # ä½¿ç”¨ä¿®æ­£å¾Œçš„åˆ†ææµç¨‹ï¼ˆé¿å… numpy é™£åˆ—å¸ƒæ—åˆ¤æ–·éŒ¯èª¤ï¼‰\n",
        "    def safe_run_analysis():\n",
        "        results = []\n",
        "        concepts = concept_provider.get_concepts_to_process()\n",
        "\n",
        "        for concept in concepts:\n",
        "            print(f\"\\nğŸ” åˆ†ææ¦‚å¿µ: {concept['base_chinese']} ({concept['translations']})\")\n",
        "            concept_global_features = {}\n",
        "\n",
        "            # å¾ translations ç”Ÿæˆæç¤ºè©\n",
        "            prompts = {}\n",
        "            for lang, translated_desc in concept['translations'].items():\n",
        "                prompts[lang] = translated_desc  # å‡è¨­ translation æ–‡å­—æœ¬èº«å°±æ˜¯æç¤ºç”¨èªå¥\n",
        "\n",
        "            for lang, prompt in prompts.items():\n",
        "                print(f\"  â¤ è™•ç†èªè¨€: {lang} | æç¤ºè©: {prompt}\")\n",
        "\n",
        "                # 1. ç”¢ç”Ÿåœ–ç‰‡\n",
        "                image = image_processor.generate_image_from_prompt(prompt)\n",
        "\n",
        "                # 2. åˆ†æåœ–åƒç‰¹å¾µ\n",
        "                global_feats, rgb_cs = text_analyzer.extract_features_and_colors(image)\n",
        "                concept_global_features[lang] = global_feats\n",
        "\n",
        "                if rgb_cs is not None and len(rgb_cs) > 0:\n",
        "                    hex_colors = [f\"#{c[0]:02x}{c[1]:02x}{c[2]:02x}\" for c in rgb_cs]\n",
        "                else:\n",
        "                    hex_colors = []\n",
        "\n",
        "                # 3. ç”Ÿæˆè§£é‡‹æ–‡å­—\n",
        "                expl_text = report_generator.generate_explanation_template(\n",
        "                    base_chinese=concept['base_chinese'],\n",
        "                    prompt_lang=lang,\n",
        "                    prompt_text=prompt,\n",
        "                    hex_colors=hex_colors\n",
        "                )\n",
        "\n",
        "                # 4. å½™ç¸½çµæœ\n",
        "                result_entry = {\n",
        "                    'concept': concept['base_chinese'],\n",
        "                    'language': lang,\n",
        "                    'prompt': prompt,\n",
        "                    'colors': hex_colors,\n",
        "                    'features': global_feats,\n",
        "                    'explanation': expl_text,\n",
        "                    'image': image\n",
        "                }\n",
        "\n",
        "                results.append(result_entry)\n",
        "\n",
        "        return results\n",
        "\n",
        "    all_results = safe_run_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kzu8lAKtYMl",
        "outputId": "c5b1a65d-fba8-49c7-8f16-aa9f15d57ee0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8:åˆå§‹å®šç¾©äº† 0 å€‹è©å½™æ¦‚å¿µã€‚\n",
            "è­¦å‘Šï¼šæ²’æœ‰å®šç¾©ä»»ä½•è©å½™æ¦‚å¿µé€²è¡Œè™•ç†ã€‚\n",
            "æ²’æœ‰ä»»ä½•æ¦‚å¿µè¢«å®šç¾©ï¼Œç„¡æ³•åŸ·è¡Œåˆ†ææµç¨‹ã€‚è«‹æª¢æŸ¥ `chinese_terms_to_analyze`ã€‚\n"
          ]
        }
      ]
    }
  ]
}