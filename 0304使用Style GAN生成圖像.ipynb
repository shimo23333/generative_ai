{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shimo23333/generative_ai/blob/main/0218%E7%95%AB%E5%87%BD%E6%95%B8%E5%9C%96%E5%9E%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>主題一：</h3>\n",
        "\n",
        "\n",
        "<b>(一)用Style GAN生成圖像</b>\n",
        "https://thisxdoesnotexist.com/\n",
        "\n",
        "\n",
        "<b>(二)為什麼現在大家不用StyleGAN改用Diffusion Model?</b>\n",
        "使用了老師上課中提到的網站嘗試生成人像，在生成的效果上我認為已經幾乎非常逼真了，但仍然有幾項缺點，例如我不能指定風格向量，無法選擇性別、種族或年齡，也無法指定燈光角度。而這種不能自由控制畫圖像的模型顯然無法滿足現在人的需求。還有一個發現是，我刷了幾十張每個人都是雙眼皮，實在是太過相似了，以及兩人以上旁邊的人有機率變形。<br>\n",
        "後來我去查詢Diffusion Model所能做的事情後，又更加認為前面推測的這件事可能是StyleGAN被淘汰的主因。看起來，擴散模型能透過提示詞來控制影像內容，還可以結合各種技術調整影像的結構與風格細節，大幅提高了創作的自由性。\n",
        "\n",
        "\n",
        "![Style GAN (2).jpg](<attachment:Style GAN (2).jpg>) \n",
        "![Style GAN (3).jpg](<attachment:Style GAN (3).jpg>) \n",
        "![Style GAN (4).jpg](<attachment:Style GAN (4).jpg>) \n",
        "![Style GAN (5).jpg](<attachment:Style GAN (5).jpg>) \n",
        "![Style GAN (6).jpg](<attachment:Style GAN (6).jpg>) \n",
        "![Style GAN (1).jpg](<attachment:Style GAN (1).jpg>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNfTXxV5V8146brw2FgAVOO",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
